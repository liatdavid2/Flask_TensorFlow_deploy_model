{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120 samples, validate on 30 samples\n",
      "Epoch 1/300\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 1.0485 - acc: 0.5000 - val_loss: 1.0399 - val_acc: 0.5000\n",
      "Epoch 2/300\n",
      "120/120 [==============================] - 0s 106us/step - loss: 1.0390 - acc: 0.5000 - val_loss: 1.0315 - val_acc: 0.5000\n",
      "Epoch 3/300\n",
      "120/120 [==============================] - 0s 122us/step - loss: 1.0296 - acc: 0.5000 - val_loss: 1.0232 - val_acc: 0.5000\n",
      "Epoch 4/300\n",
      "120/120 [==============================] - 0s 121us/step - loss: 1.0195 - acc: 0.5083 - val_loss: 1.0147 - val_acc: 0.5000\n",
      "Epoch 5/300\n",
      "120/120 [==============================] - 0s 122us/step - loss: 1.0098 - acc: 0.4917 - val_loss: 1.0061 - val_acc: 0.5000\n",
      "Epoch 6/300\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.9993 - acc: 0.4833 - val_loss: 0.9974 - val_acc: 0.4667\n",
      "Epoch 7/300\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.9894 - acc: 0.4833 - val_loss: 0.9885 - val_acc: 0.5000\n",
      "Epoch 8/300\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.9792 - acc: 0.4917 - val_loss: 0.9795 - val_acc: 0.5667\n",
      "Epoch 9/300\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.9683 - acc: 0.5083 - val_loss: 0.9706 - val_acc: 0.5667\n",
      "Epoch 10/300\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.9589 - acc: 0.6417 - val_loss: 0.9613 - val_acc: 0.6000\n",
      "Epoch 11/300\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.9480 - acc: 0.6750 - val_loss: 0.9521 - val_acc: 0.6000\n",
      "Epoch 12/300\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.9371 - acc: 0.6750 - val_loss: 0.9431 - val_acc: 0.6000\n",
      "Epoch 13/300\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.9268 - acc: 0.6750 - val_loss: 0.9338 - val_acc: 0.6000\n",
      "Epoch 14/300\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.9156 - acc: 0.6750 - val_loss: 0.9248 - val_acc: 0.6000\n",
      "Epoch 15/300\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.9053 - acc: 0.6750 - val_loss: 0.9156 - val_acc: 0.6000\n",
      "Epoch 16/300\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.8951 - acc: 0.6750 - val_loss: 0.9064 - val_acc: 0.6000\n",
      "Epoch 17/300\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.8839 - acc: 0.6750 - val_loss: 0.8973 - val_acc: 0.6000\n",
      "Epoch 18/300\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.8732 - acc: 0.6750 - val_loss: 0.8885 - val_acc: 0.6000\n",
      "Epoch 19/300\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.8630 - acc: 0.6750 - val_loss: 0.8795 - val_acc: 0.6000\n",
      "Epoch 20/300\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.8529 - acc: 0.6750 - val_loss: 0.8706 - val_acc: 0.6000\n",
      "Epoch 21/300\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.8423 - acc: 0.6750 - val_loss: 0.8619 - val_acc: 0.6000\n",
      "Epoch 22/300\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.8319 - acc: 0.6750 - val_loss: 0.8533 - val_acc: 0.6000\n",
      "Epoch 23/300\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.8220 - acc: 0.6750 - val_loss: 0.8448 - val_acc: 0.6000\n",
      "Epoch 24/300\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.8126 - acc: 0.6750 - val_loss: 0.8363 - val_acc: 0.6000\n",
      "Epoch 25/300\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.8020 - acc: 0.6750 - val_loss: 0.8280 - val_acc: 0.6000\n",
      "Epoch 26/300\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.7925 - acc: 0.6750 - val_loss: 0.8200 - val_acc: 0.6000\n",
      "Epoch 27/300\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.7831 - acc: 0.6750 - val_loss: 0.8122 - val_acc: 0.6000\n",
      "Epoch 28/300\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.7738 - acc: 0.6750 - val_loss: 0.8044 - val_acc: 0.6000\n",
      "Epoch 29/300\n",
      "120/120 [==============================] - 0s 211us/step - loss: 0.7646 - acc: 0.6750 - val_loss: 0.7968 - val_acc: 0.6000\n",
      "Epoch 30/300\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.7559 - acc: 0.6750 - val_loss: 0.7893 - val_acc: 0.6000\n",
      "Epoch 31/300\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.7469 - acc: 0.6750 - val_loss: 0.7820 - val_acc: 0.6000\n",
      "Epoch 32/300\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.7384 - acc: 0.6750 - val_loss: 0.7750 - val_acc: 0.6000\n",
      "Epoch 33/300\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.7300 - acc: 0.6750 - val_loss: 0.7681 - val_acc: 0.6000\n",
      "Epoch 34/300\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.7217 - acc: 0.6750 - val_loss: 0.7612 - val_acc: 0.6000\n",
      "Epoch 35/300\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.7136 - acc: 0.6833 - val_loss: 0.7546 - val_acc: 0.6000\n",
      "Epoch 36/300\n",
      "120/120 [==============================] - 0s 77us/step - loss: 0.7058 - acc: 0.6833 - val_loss: 0.7482 - val_acc: 0.6000\n",
      "Epoch 37/300\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.6983 - acc: 0.6833 - val_loss: 0.7419 - val_acc: 0.6000\n",
      "Epoch 38/300\n",
      "120/120 [==============================] - 0s 81us/step - loss: 0.6907 - acc: 0.6833 - val_loss: 0.7358 - val_acc: 0.6000\n",
      "Epoch 39/300\n",
      "120/120 [==============================] - 0s 81us/step - loss: 0.6837 - acc: 0.6833 - val_loss: 0.7299 - val_acc: 0.6000\n",
      "Epoch 40/300\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.6765 - acc: 0.6833 - val_loss: 0.7241 - val_acc: 0.6333\n",
      "Epoch 41/300\n",
      "120/120 [==============================] - 0s 73us/step - loss: 0.6696 - acc: 0.6833 - val_loss: 0.7185 - val_acc: 0.6667\n",
      "Epoch 42/300\n",
      "120/120 [==============================] - 0s 89us/step - loss: 0.6631 - acc: 0.6833 - val_loss: 0.7130 - val_acc: 0.7000\n",
      "Epoch 43/300\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.6567 - acc: 0.7000 - val_loss: 0.7077 - val_acc: 0.7000\n",
      "Epoch 44/300\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.6503 - acc: 0.7000 - val_loss: 0.7024 - val_acc: 0.7000\n",
      "Epoch 45/300\n",
      "120/120 [==============================] - 0s 81us/step - loss: 0.6440 - acc: 0.7000 - val_loss: 0.6973 - val_acc: 0.7000\n",
      "Epoch 46/300\n",
      "120/120 [==============================] - 0s 89us/step - loss: 0.6379 - acc: 0.7000 - val_loss: 0.6923 - val_acc: 0.7000\n",
      "Epoch 47/300\n",
      "120/120 [==============================] - 0s 73us/step - loss: 0.6324 - acc: 0.7000 - val_loss: 0.6875 - val_acc: 0.7000\n",
      "Epoch 48/300\n",
      "120/120 [==============================] - 0s 73us/step - loss: 0.6266 - acc: 0.7000 - val_loss: 0.6828 - val_acc: 0.7000\n",
      "Epoch 49/300\n",
      "120/120 [==============================] - 0s 89us/step - loss: 0.6211 - acc: 0.7000 - val_loss: 0.6782 - val_acc: 0.7000\n",
      "Epoch 50/300\n",
      "120/120 [==============================] - 0s 90us/step - loss: 0.6156 - acc: 0.7417 - val_loss: 0.6737 - val_acc: 0.8000\n",
      "Epoch 51/300\n",
      "120/120 [==============================] - 0s 90us/step - loss: 0.6104 - acc: 0.7500 - val_loss: 0.6694 - val_acc: 0.8000\n",
      "Epoch 52/300\n",
      "120/120 [==============================] - 0s 73us/step - loss: 0.6053 - acc: 0.7500 - val_loss: 0.6651 - val_acc: 0.8000\n",
      "Epoch 53/300\n",
      "120/120 [==============================] - 0s 81us/step - loss: 0.6004 - acc: 0.7500 - val_loss: 0.6610 - val_acc: 0.8000\n",
      "Epoch 54/300\n",
      "120/120 [==============================] - 0s 90us/step - loss: 0.5955 - acc: 0.7500 - val_loss: 0.6570 - val_acc: 0.8000\n",
      "Epoch 55/300\n",
      "120/120 [==============================] - 0s 81us/step - loss: 0.5908 - acc: 0.7500 - val_loss: 0.6530 - val_acc: 0.8000\n",
      "Epoch 56/300\n",
      "120/120 [==============================] - 0s 72us/step - loss: 0.5865 - acc: 0.7500 - val_loss: 0.6492 - val_acc: 0.8000\n",
      "Epoch 57/300\n",
      "120/120 [==============================] - 0s 57us/step - loss: 0.5818 - acc: 0.7500 - val_loss: 0.6455 - val_acc: 0.8000\n",
      "Epoch 58/300\n",
      "120/120 [==============================] - 0s 65us/step - loss: 0.5775 - acc: 0.7500 - val_loss: 0.6418 - val_acc: 0.8000\n",
      "Epoch 59/300\n",
      "120/120 [==============================] - 0s 57us/step - loss: 0.5734 - acc: 0.7500 - val_loss: 0.6382 - val_acc: 0.8000\n",
      "Epoch 60/300\n",
      "120/120 [==============================] - 0s 73us/step - loss: 0.5693 - acc: 0.7583 - val_loss: 0.6346 - val_acc: 0.8000\n",
      "Epoch 61/300\n",
      "120/120 [==============================] - 0s 67us/step - loss: 0.5653 - acc: 0.7583 - val_loss: 0.6312 - val_acc: 0.8000\n",
      "Epoch 62/300\n",
      "120/120 [==============================] - 0s 75us/step - loss: 0.5614 - acc: 0.7583 - val_loss: 0.6278 - val_acc: 0.8000\n",
      "Epoch 63/300\n",
      "120/120 [==============================] - 0s 65us/step - loss: 0.5577 - acc: 0.7667 - val_loss: 0.6245 - val_acc: 0.8000\n",
      "Epoch 64/300\n",
      "120/120 [==============================] - 0s 57us/step - loss: 0.5540 - acc: 0.7667 - val_loss: 0.6213 - val_acc: 0.8000\n",
      "Epoch 65/300\n",
      "120/120 [==============================] - 0s 57us/step - loss: 0.5504 - acc: 0.7667 - val_loss: 0.6180 - val_acc: 0.8000\n",
      "Epoch 66/300\n",
      "120/120 [==============================] - 0s 75us/step - loss: 0.5469 - acc: 0.7750 - val_loss: 0.6149 - val_acc: 0.8000\n",
      "Epoch 67/300\n",
      "120/120 [==============================] - 0s 81us/step - loss: 0.5435 - acc: 0.7750 - val_loss: 0.6119 - val_acc: 0.8000\n",
      "Epoch 68/300\n",
      "120/120 [==============================] - 0s 73us/step - loss: 0.5402 - acc: 0.7750 - val_loss: 0.6089 - val_acc: 0.8000\n",
      "Epoch 69/300\n",
      "120/120 [==============================] - 0s 89us/step - loss: 0.5369 - acc: 0.7917 - val_loss: 0.6059 - val_acc: 0.8000\n",
      "Epoch 70/300\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.5337 - acc: 0.7917 - val_loss: 0.6030 - val_acc: 0.8000\n",
      "Epoch 71/300\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.5306 - acc: 0.8000 - val_loss: 0.6002 - val_acc: 0.8000\n",
      "Epoch 72/300\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.5276 - acc: 0.8000 - val_loss: 0.5974 - val_acc: 0.8000\n",
      "Epoch 73/300\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.5246 - acc: 0.8000 - val_loss: 0.5947 - val_acc: 0.8000\n",
      "Epoch 74/300\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.5218 - acc: 0.8083 - val_loss: 0.5920 - val_acc: 0.8333\n",
      "Epoch 75/300\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.5189 - acc: 0.8083 - val_loss: 0.5893 - val_acc: 0.8333\n",
      "Epoch 76/300\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.5161 - acc: 0.8083 - val_loss: 0.5867 - val_acc: 0.8333\n",
      "Epoch 77/300\n",
      "120/120 [==============================] - 0s 89us/step - loss: 0.5134 - acc: 0.8167 - val_loss: 0.5841 - val_acc: 0.8333\n",
      "Epoch 78/300\n",
      "120/120 [==============================] - 0s 89us/step - loss: 0.5108 - acc: 0.8167 - val_loss: 0.5816 - val_acc: 0.8333\n",
      "Epoch 79/300\n",
      "120/120 [==============================] - 0s 74us/step - loss: 0.5082 - acc: 0.8167 - val_loss: 0.5791 - val_acc: 0.8333\n",
      "Epoch 80/300\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.5057 - acc: 0.8167 - val_loss: 0.5766 - val_acc: 0.8333\n",
      "Epoch 81/300\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.5031 - acc: 0.8167 - val_loss: 0.5742 - val_acc: 0.8333\n",
      "Epoch 82/300\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.5007 - acc: 0.8167 - val_loss: 0.5718 - val_acc: 0.8333\n",
      "Epoch 83/300\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.4983 - acc: 0.8167 - val_loss: 0.5695 - val_acc: 0.8333\n",
      "Epoch 84/300\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.4960 - acc: 0.8167 - val_loss: 0.5672 - val_acc: 0.8333\n",
      "Epoch 85/300\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.4937 - acc: 0.8167 - val_loss: 0.5649 - val_acc: 0.8333\n",
      "Epoch 86/300\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.4914 - acc: 0.8167 - val_loss: 0.5626 - val_acc: 0.8333\n",
      "Epoch 87/300\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.4892 - acc: 0.8167 - val_loss: 0.5604 - val_acc: 0.8333\n",
      "Epoch 88/300\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.4870 - acc: 0.8167 - val_loss: 0.5581 - val_acc: 0.8333\n",
      "Epoch 89/300\n",
      "120/120 [==============================] - 0s 81us/step - loss: 0.4848 - acc: 0.8167 - val_loss: 0.5560 - val_acc: 0.8333\n",
      "Epoch 90/300\n",
      "120/120 [==============================] - 0s 86us/step - loss: 0.4827 - acc: 0.8167 - val_loss: 0.5538 - val_acc: 0.8333\n",
      "Epoch 91/300\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.4805 - acc: 0.8167 - val_loss: 0.5516 - val_acc: 0.8333\n",
      "Epoch 92/300\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.4786 - acc: 0.8167 - val_loss: 0.5494 - val_acc: 0.8333\n",
      "Epoch 93/300\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.4765 - acc: 0.8250 - val_loss: 0.5473 - val_acc: 0.8333\n",
      "Epoch 94/300\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.4746 - acc: 0.8250 - val_loss: 0.5452 - val_acc: 0.8333\n",
      "Epoch 95/300\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.4726 - acc: 0.8250 - val_loss: 0.5432 - val_acc: 0.8333\n",
      "Epoch 96/300\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.4707 - acc: 0.8250 - val_loss: 0.5412 - val_acc: 0.8333\n",
      "Epoch 97/300\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4739 - acc: 0.843 - 0s 94us/step - loss: 0.4688 - acc: 0.8250 - val_loss: 0.5392 - val_acc: 0.8333\n",
      "Epoch 98/300\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.4669 - acc: 0.8250 - val_loss: 0.5373 - val_acc: 0.8333\n",
      "Epoch 99/300\n",
      "120/120 [==============================] - 0s 81us/step - loss: 0.4651 - acc: 0.8250 - val_loss: 0.5353 - val_acc: 0.8333\n",
      "Epoch 100/300\n",
      "120/120 [==============================] - 0s 89us/step - loss: 0.4633 - acc: 0.8250 - val_loss: 0.5333 - val_acc: 0.8333\n",
      "Epoch 101/300\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.4614 - acc: 0.8250 - val_loss: 0.5315 - val_acc: 0.8333\n",
      "Epoch 102/300\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.4597 - acc: 0.8250 - val_loss: 0.5296 - val_acc: 0.8333\n",
      "Epoch 103/300\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.4580 - acc: 0.8250 - val_loss: 0.5277 - val_acc: 0.8333\n",
      "Epoch 104/300\n",
      "120/120 [==============================] - 0s 81us/step - loss: 0.4562 - acc: 0.8250 - val_loss: 0.5258 - val_acc: 0.8333\n",
      "Epoch 105/300\n",
      "120/120 [==============================] - 0s 73us/step - loss: 0.4546 - acc: 0.8250 - val_loss: 0.5240 - val_acc: 0.8333\n",
      "Epoch 106/300\n",
      "120/120 [==============================] - 0s 89us/step - loss: 0.4529 - acc: 0.8250 - val_loss: 0.5222 - val_acc: 0.8333\n",
      "Epoch 107/300\n",
      "120/120 [==============================] - 0s 92us/step - loss: 0.4512 - acc: 0.8250 - val_loss: 0.5203 - val_acc: 0.8333\n",
      "Epoch 108/300\n",
      "120/120 [==============================] - 0s 73us/step - loss: 0.4495 - acc: 0.8250 - val_loss: 0.5185 - val_acc: 0.8333\n",
      "Epoch 109/300\n",
      "120/120 [==============================] - 0s 81us/step - loss: 0.4479 - acc: 0.8250 - val_loss: 0.5167 - val_acc: 0.8333\n",
      "Epoch 110/300\n",
      "120/120 [==============================] - 0s 77us/step - loss: 0.4464 - acc: 0.8250 - val_loss: 0.5149 - val_acc: 0.8333\n",
      "Epoch 111/300\n",
      "120/120 [==============================] - 0s 81us/step - loss: 0.4447 - acc: 0.8250 - val_loss: 0.5132 - val_acc: 0.8333\n",
      "Epoch 112/300\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.4432 - acc: 0.8250 - val_loss: 0.5115 - val_acc: 0.8333\n",
      "Epoch 113/300\n",
      "120/120 [==============================] - 0s 89us/step - loss: 0.4416 - acc: 0.8250 - val_loss: 0.5098 - val_acc: 0.8333\n",
      "Epoch 114/300\n",
      "120/120 [==============================] - 0s 65us/step - loss: 0.4401 - acc: 0.8250 - val_loss: 0.5081 - val_acc: 0.8333\n",
      "Epoch 115/300\n",
      "120/120 [==============================] - 0s 65us/step - loss: 0.4386 - acc: 0.8250 - val_loss: 0.5063 - val_acc: 0.8333\n",
      "Epoch 116/300\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.4371 - acc: 0.8250 - val_loss: 0.5047 - val_acc: 0.8333\n",
      "Epoch 117/300\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.4356 - acc: 0.8250 - val_loss: 0.5030 - val_acc: 0.8333\n",
      "Epoch 118/300\n",
      "120/120 [==============================] - 0s 301us/step - loss: 0.4342 - acc: 0.8250 - val_loss: 0.5013 - val_acc: 0.8333\n",
      "Epoch 119/300\n",
      "120/120 [==============================] - 0s 207us/step - loss: 0.4327 - acc: 0.8250 - val_loss: 0.4997 - val_acc: 0.8333\n",
      "Epoch 120/300\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.4312 - acc: 0.8250 - val_loss: 0.4981 - val_acc: 0.8333\n",
      "Epoch 121/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 89us/step - loss: 0.4299 - acc: 0.8250 - val_loss: 0.4965 - val_acc: 0.8333\n",
      "Epoch 122/300\n",
      "120/120 [==============================] - 0s 89us/step - loss: 0.4284 - acc: 0.8250 - val_loss: 0.4949 - val_acc: 0.8333\n",
      "Epoch 123/300\n",
      "120/120 [==============================] - 0s 73us/step - loss: 0.4270 - acc: 0.8250 - val_loss: 0.4933 - val_acc: 0.8333\n",
      "Epoch 124/300\n",
      "120/120 [==============================] - 0s 75us/step - loss: 0.4257 - acc: 0.8250 - val_loss: 0.4917 - val_acc: 0.8333\n",
      "Epoch 125/300\n",
      "120/120 [==============================] - 0s 81us/step - loss: 0.4243 - acc: 0.8250 - val_loss: 0.4902 - val_acc: 0.8333\n",
      "Epoch 126/300\n",
      "120/120 [==============================] - 0s 65us/step - loss: 0.4230 - acc: 0.8250 - val_loss: 0.4886 - val_acc: 0.8333\n",
      "Epoch 127/300\n",
      "120/120 [==============================] - 0s 88us/step - loss: 0.4216 - acc: 0.8250 - val_loss: 0.4870 - val_acc: 0.8333\n",
      "Epoch 128/300\n",
      "120/120 [==============================] - 0s 89us/step - loss: 0.4202 - acc: 0.8250 - val_loss: 0.4855 - val_acc: 0.8333\n",
      "Epoch 129/300\n",
      "120/120 [==============================] - 0s 65us/step - loss: 0.4189 - acc: 0.8250 - val_loss: 0.4840 - val_acc: 0.8333\n",
      "Epoch 130/300\n",
      "120/120 [==============================] - 0s 81us/step - loss: 0.4176 - acc: 0.8250 - val_loss: 0.4826 - val_acc: 0.8333\n",
      "Epoch 131/300\n",
      "120/120 [==============================] - 0s 81us/step - loss: 0.4164 - acc: 0.8250 - val_loss: 0.4811 - val_acc: 0.8333\n",
      "Epoch 132/300\n",
      "120/120 [==============================] - 0s 73us/step - loss: 0.4151 - acc: 0.8250 - val_loss: 0.4796 - val_acc: 0.8333\n",
      "Epoch 133/300\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.4139 - acc: 0.8250 - val_loss: 0.4780 - val_acc: 0.8333\n",
      "Epoch 134/300\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.4125 - acc: 0.8250 - val_loss: 0.4766 - val_acc: 0.8333\n",
      "Epoch 135/300\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.4113 - acc: 0.8250 - val_loss: 0.4751 - val_acc: 0.8333\n",
      "Epoch 136/300\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.4100 - acc: 0.8250 - val_loss: 0.4737 - val_acc: 0.8333\n",
      "Epoch 137/300\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.4089 - acc: 0.8250 - val_loss: 0.4722 - val_acc: 0.8333\n",
      "Epoch 138/300\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.4076 - acc: 0.8250 - val_loss: 0.4708 - val_acc: 0.8333\n",
      "Epoch 139/300\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.4065 - acc: 0.8333 - val_loss: 0.4693 - val_acc: 0.8333\n",
      "Epoch 140/300\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.4052 - acc: 0.8417 - val_loss: 0.4679 - val_acc: 0.8333\n",
      "Epoch 141/300\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.4040 - acc: 0.8417 - val_loss: 0.4665 - val_acc: 0.8333\n",
      "Epoch 142/300\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.4029 - acc: 0.8417 - val_loss: 0.4651 - val_acc: 0.8333\n",
      "Epoch 143/300\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.4017 - acc: 0.8417 - val_loss: 0.4637 - val_acc: 0.8333\n",
      "Epoch 144/300\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.4006 - acc: 0.8417 - val_loss: 0.4623 - val_acc: 0.8333\n",
      "Epoch 145/300\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.3995 - acc: 0.8417 - val_loss: 0.4609 - val_acc: 0.8333\n",
      "Epoch 146/300\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.3983 - acc: 0.8417 - val_loss: 0.4595 - val_acc: 0.8333\n",
      "Epoch 147/300\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.3972 - acc: 0.8417 - val_loss: 0.4583 - val_acc: 0.8333\n",
      "Epoch 148/300\n",
      "120/120 [==============================] - 0s 81us/step - loss: 0.3960 - acc: 0.8417 - val_loss: 0.4570 - val_acc: 0.8333\n",
      "Epoch 149/300\n",
      "120/120 [==============================] - 0s 81us/step - loss: 0.3948 - acc: 0.8417 - val_loss: 0.4556 - val_acc: 0.8333\n",
      "Epoch 150/300\n",
      "120/120 [==============================] - 0s 82us/step - loss: 0.3939 - acc: 0.8500 - val_loss: 0.4542 - val_acc: 0.8333\n",
      "Epoch 151/300\n",
      "120/120 [==============================] - 0s 66us/step - loss: 0.3927 - acc: 0.8500 - val_loss: 0.4529 - val_acc: 0.8333\n",
      "Epoch 152/300\n",
      "120/120 [==============================] - 0s 65us/step - loss: 0.3916 - acc: 0.8500 - val_loss: 0.4516 - val_acc: 0.8333\n",
      "Epoch 153/300\n",
      "120/120 [==============================] - 0s 73us/step - loss: 0.3905 - acc: 0.8500 - val_loss: 0.4503 - val_acc: 0.8333\n",
      "Epoch 154/300\n",
      "120/120 [==============================] - 0s 65us/step - loss: 0.3894 - acc: 0.8500 - val_loss: 0.4490 - val_acc: 0.8333\n",
      "Epoch 155/300\n",
      "120/120 [==============================] - 0s 74us/step - loss: 0.3884 - acc: 0.8500 - val_loss: 0.4478 - val_acc: 0.8333\n",
      "Epoch 156/300\n",
      "120/120 [==============================] - 0s 73us/step - loss: 0.3873 - acc: 0.8500 - val_loss: 0.4466 - val_acc: 0.8333\n",
      "Epoch 157/300\n",
      "120/120 [==============================] - 0s 67us/step - loss: 0.3864 - acc: 0.8500 - val_loss: 0.4452 - val_acc: 0.8333\n",
      "Epoch 158/300\n",
      "120/120 [==============================] - 0s 84us/step - loss: 0.3852 - acc: 0.8500 - val_loss: 0.4441 - val_acc: 0.8333\n",
      "Epoch 159/300\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.3843 - acc: 0.8500 - val_loss: 0.4428 - val_acc: 0.8333\n",
      "Epoch 160/300\n",
      "120/120 [==============================] - 0s 73us/step - loss: 0.3833 - acc: 0.8500 - val_loss: 0.4416 - val_acc: 0.8333\n",
      "Epoch 161/300\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.3822 - acc: 0.8500 - val_loss: 0.4404 - val_acc: 0.8333\n",
      "Epoch 162/300\n",
      "120/120 [==============================] - 0s 81us/step - loss: 0.3812 - acc: 0.8500 - val_loss: 0.4392 - val_acc: 0.8333\n",
      "Epoch 163/300\n",
      "120/120 [==============================] - 0s 65us/step - loss: 0.3802 - acc: 0.8500 - val_loss: 0.4380 - val_acc: 0.8333\n",
      "Epoch 164/300\n",
      "120/120 [==============================] - 0s 81us/step - loss: 0.3792 - acc: 0.8500 - val_loss: 0.4368 - val_acc: 0.8333\n",
      "Epoch 165/300\n",
      "120/120 [==============================] - 0s 89us/step - loss: 0.3782 - acc: 0.8500 - val_loss: 0.4356 - val_acc: 0.8333\n",
      "Epoch 166/300\n",
      "120/120 [==============================] - 0s 79us/step - loss: 0.3772 - acc: 0.8417 - val_loss: 0.4345 - val_acc: 0.8333\n",
      "Epoch 167/300\n",
      "120/120 [==============================] - 0s 63us/step - loss: 0.3762 - acc: 0.8417 - val_loss: 0.4333 - val_acc: 0.8333\n",
      "Epoch 168/300\n",
      "120/120 [==============================] - 0s 89us/step - loss: 0.3753 - acc: 0.8417 - val_loss: 0.4321 - val_acc: 0.8333\n",
      "Epoch 169/300\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.3743 - acc: 0.8417 - val_loss: 0.4309 - val_acc: 0.8333\n",
      "Epoch 170/300\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3983 - acc: 0.781 - 0s 114us/step - loss: 0.3734 - acc: 0.8417 - val_loss: 0.4298 - val_acc: 0.8333\n",
      "Epoch 171/300\n",
      "120/120 [==============================] - 0s 81us/step - loss: 0.3724 - acc: 0.8500 - val_loss: 0.4286 - val_acc: 0.8667\n",
      "Epoch 172/300\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.3715 - acc: 0.8500 - val_loss: 0.4274 - val_acc: 0.8667\n",
      "Epoch 173/300\n",
      "120/120 [==============================] - 0s 90us/step - loss: 0.3705 - acc: 0.8500 - val_loss: 0.4263 - val_acc: 0.8667\n",
      "Epoch 174/300\n",
      "120/120 [==============================] - 0s 81us/step - loss: 0.3696 - acc: 0.8500 - val_loss: 0.4252 - val_acc: 0.8667\n",
      "Epoch 175/300\n",
      "120/120 [==============================] - 0s 76us/step - loss: 0.3686 - acc: 0.8500 - val_loss: 0.4240 - val_acc: 0.8667\n",
      "Epoch 176/300\n",
      "120/120 [==============================] - 0s 81us/step - loss: 0.3678 - acc: 0.8500 - val_loss: 0.4229 - val_acc: 0.8667\n",
      "Epoch 177/300\n",
      "120/120 [==============================] - 0s 81us/step - loss: 0.3668 - acc: 0.8500 - val_loss: 0.4218 - val_acc: 0.8667\n",
      "Epoch 178/300\n",
      "120/120 [==============================] - 0s 81us/step - loss: 0.3659 - acc: 0.8500 - val_loss: 0.4207 - val_acc: 0.8667\n",
      "Epoch 179/300\n",
      "120/120 [==============================] - 0s 65us/step - loss: 0.3650 - acc: 0.8500 - val_loss: 0.4197 - val_acc: 0.8667\n",
      "Epoch 180/300\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3296 - acc: 0.906 - 0s 57us/step - loss: 0.3641 - acc: 0.8500 - val_loss: 0.4186 - val_acc: 0.8667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/300\n",
      "120/120 [==============================] - 0s 57us/step - loss: 0.3632 - acc: 0.8500 - val_loss: 0.4175 - val_acc: 0.8667\n",
      "Epoch 182/300\n",
      "120/120 [==============================] - 0s 89us/step - loss: 0.3624 - acc: 0.8500 - val_loss: 0.4164 - val_acc: 0.8667\n",
      "Epoch 183/300\n",
      "120/120 [==============================] - 0s 73us/step - loss: 0.3615 - acc: 0.8583 - val_loss: 0.4153 - val_acc: 0.8667\n",
      "Epoch 184/300\n",
      "120/120 [==============================] - 0s 90us/step - loss: 0.3606 - acc: 0.8583 - val_loss: 0.4142 - val_acc: 0.8667\n",
      "Epoch 185/300\n",
      "120/120 [==============================] - 0s 80us/step - loss: 0.3597 - acc: 0.8583 - val_loss: 0.4131 - val_acc: 0.8667\n",
      "Epoch 186/300\n",
      "120/120 [==============================] - 0s 81us/step - loss: 0.3589 - acc: 0.8583 - val_loss: 0.4120 - val_acc: 0.8667\n",
      "Epoch 187/300\n",
      "120/120 [==============================] - 0s 81us/step - loss: 0.3580 - acc: 0.8583 - val_loss: 0.4109 - val_acc: 0.8667\n",
      "Epoch 188/300\n",
      "120/120 [==============================] - 0s 89us/step - loss: 0.3572 - acc: 0.8583 - val_loss: 0.4099 - val_acc: 0.8667\n",
      "Epoch 189/300\n",
      "120/120 [==============================] - 0s 89us/step - loss: 0.3563 - acc: 0.8583 - val_loss: 0.4088 - val_acc: 0.8667\n",
      "Epoch 190/300\n",
      "120/120 [==============================] - 0s 65us/step - loss: 0.3555 - acc: 0.8583 - val_loss: 0.4077 - val_acc: 0.8667\n",
      "Epoch 191/300\n",
      "120/120 [==============================] - 0s 90us/step - loss: 0.3547 - acc: 0.8750 - val_loss: 0.4067 - val_acc: 0.8667\n",
      "Epoch 192/300\n",
      "120/120 [==============================] - 0s 89us/step - loss: 0.3539 - acc: 0.8750 - val_loss: 0.4056 - val_acc: 0.8667\n",
      "Epoch 193/300\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.3530 - acc: 0.8750 - val_loss: 0.4046 - val_acc: 0.8667\n",
      "Epoch 194/300\n",
      "120/120 [==============================] - 0s 73us/step - loss: 0.3522 - acc: 0.8750 - val_loss: 0.4036 - val_acc: 0.8667\n",
      "Epoch 195/300\n",
      "120/120 [==============================] - 0s 89us/step - loss: 0.3514 - acc: 0.8750 - val_loss: 0.4026 - val_acc: 0.8667\n",
      "Epoch 196/300\n",
      "120/120 [==============================] - 0s 73us/step - loss: 0.3506 - acc: 0.8750 - val_loss: 0.4016 - val_acc: 0.8667\n",
      "Epoch 197/300\n",
      "120/120 [==============================] - 0s 74us/step - loss: 0.3498 - acc: 0.8750 - val_loss: 0.4006 - val_acc: 0.8667\n",
      "Epoch 198/300\n",
      "120/120 [==============================] - 0s 65us/step - loss: 0.3490 - acc: 0.8750 - val_loss: 0.3996 - val_acc: 0.8667\n",
      "Epoch 199/300\n",
      "120/120 [==============================] - 0s 73us/step - loss: 0.3482 - acc: 0.8750 - val_loss: 0.3986 - val_acc: 0.8667\n",
      "Epoch 200/300\n",
      "120/120 [==============================] - 0s 65us/step - loss: 0.3474 - acc: 0.8750 - val_loss: 0.3976 - val_acc: 0.8667\n",
      "Epoch 201/300\n",
      "120/120 [==============================] - 0s 81us/step - loss: 0.3467 - acc: 0.8750 - val_loss: 0.3966 - val_acc: 0.8667\n",
      "Epoch 202/300\n",
      "120/120 [==============================] - 0s 73us/step - loss: 0.3459 - acc: 0.8750 - val_loss: 0.3956 - val_acc: 0.8667\n",
      "Epoch 203/300\n",
      "120/120 [==============================] - 0s 65us/step - loss: 0.3451 - acc: 0.8750 - val_loss: 0.3946 - val_acc: 0.8667\n",
      "Epoch 204/300\n",
      "120/120 [==============================] - 0s 81us/step - loss: 0.3443 - acc: 0.8750 - val_loss: 0.3937 - val_acc: 0.8667\n",
      "Epoch 205/300\n",
      "120/120 [==============================] - 0s 73us/step - loss: 0.3435 - acc: 0.8750 - val_loss: 0.3927 - val_acc: 0.8667\n",
      "Epoch 206/300\n",
      "120/120 [==============================] - 0s 90us/step - loss: 0.3427 - acc: 0.8750 - val_loss: 0.3918 - val_acc: 0.8667\n",
      "Epoch 207/300\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.3420 - acc: 0.8750 - val_loss: 0.3908 - val_acc: 0.8667\n",
      "Epoch 208/300\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.3412 - acc: 0.8750 - val_loss: 0.3898 - val_acc: 0.9000\n",
      "Epoch 209/300\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.3404 - acc: 0.8750 - val_loss: 0.3888 - val_acc: 0.9000\n",
      "Epoch 210/300\n",
      "120/120 [==============================] - 0s 89us/step - loss: 0.3398 - acc: 0.8750 - val_loss: 0.3879 - val_acc: 0.9000\n",
      "Epoch 211/300\n",
      "120/120 [==============================] - 0s 81us/step - loss: 0.3390 - acc: 0.8750 - val_loss: 0.3869 - val_acc: 0.9000\n",
      "Epoch 212/300\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.3382 - acc: 0.8750 - val_loss: 0.3859 - val_acc: 0.9000\n",
      "Epoch 213/300\n",
      "120/120 [==============================] - 0s 89us/step - loss: 0.3375 - acc: 0.8750 - val_loss: 0.3850 - val_acc: 0.9000\n",
      "Epoch 214/300\n",
      "120/120 [==============================] - 0s 89us/step - loss: 0.3367 - acc: 0.8750 - val_loss: 0.3840 - val_acc: 0.9000\n",
      "Epoch 215/300\n",
      "120/120 [==============================] - 0s 73us/step - loss: 0.3360 - acc: 0.8750 - val_loss: 0.3831 - val_acc: 0.9000\n",
      "Epoch 216/300\n",
      "120/120 [==============================] - 0s 81us/step - loss: 0.3353 - acc: 0.8750 - val_loss: 0.3821 - val_acc: 0.9000\n",
      "Epoch 217/300\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.3346 - acc: 0.8750 - val_loss: 0.3812 - val_acc: 0.9000\n",
      "Epoch 218/300\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.3338 - acc: 0.8750 - val_loss: 0.3802 - val_acc: 0.9000\n",
      "Epoch 219/300\n",
      "120/120 [==============================] - 0s 89us/step - loss: 0.3331 - acc: 0.8750 - val_loss: 0.3793 - val_acc: 0.9000\n",
      "Epoch 220/300\n",
      "120/120 [==============================] - 0s 65us/step - loss: 0.3324 - acc: 0.8750 - val_loss: 0.3784 - val_acc: 0.9000\n",
      "Epoch 221/300\n",
      "120/120 [==============================] - 0s 73us/step - loss: 0.3317 - acc: 0.8750 - val_loss: 0.3775 - val_acc: 0.9000\n",
      "Epoch 222/300\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.3310 - acc: 0.8750 - val_loss: 0.3766 - val_acc: 0.9000\n",
      "Epoch 223/300\n",
      "120/120 [==============================] - 0s 89us/step - loss: 0.3302 - acc: 0.8750 - val_loss: 0.3758 - val_acc: 0.9333\n",
      "Epoch 224/300\n",
      "120/120 [==============================] - 0s 63us/step - loss: 0.3296 - acc: 0.8750 - val_loss: 0.3749 - val_acc: 0.9333\n",
      "Epoch 225/300\n",
      "120/120 [==============================] - 0s 81us/step - loss: 0.3288 - acc: 0.8750 - val_loss: 0.3740 - val_acc: 0.9333\n",
      "Epoch 226/300\n",
      "120/120 [==============================] - 0s 88us/step - loss: 0.3281 - acc: 0.8750 - val_loss: 0.3731 - val_acc: 0.9333\n",
      "Epoch 227/300\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.3275 - acc: 0.8750 - val_loss: 0.3722 - val_acc: 0.9333\n",
      "Epoch 228/300\n",
      "120/120 [==============================] - 0s 78us/step - loss: 0.3268 - acc: 0.8750 - val_loss: 0.3713 - val_acc: 0.9333\n",
      "Epoch 229/300\n",
      "120/120 [==============================] - 0s 81us/step - loss: 0.3261 - acc: 0.8750 - val_loss: 0.3705 - val_acc: 0.9333\n",
      "Epoch 230/300\n",
      "120/120 [==============================] - 0s 81us/step - loss: 0.3254 - acc: 0.8750 - val_loss: 0.3696 - val_acc: 0.9333\n",
      "Epoch 231/300\n",
      "120/120 [==============================] - 0s 78us/step - loss: 0.3247 - acc: 0.8750 - val_loss: 0.3687 - val_acc: 0.9333\n",
      "Epoch 232/300\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.3241 - acc: 0.8750 - val_loss: 0.3679 - val_acc: 0.9333\n",
      "Epoch 233/300\n",
      "120/120 [==============================] - 0s 81us/step - loss: 0.3234 - acc: 0.8750 - val_loss: 0.3670 - val_acc: 0.9333\n",
      "Epoch 234/300\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.3227 - acc: 0.8750 - val_loss: 0.3662 - val_acc: 0.9333\n",
      "Epoch 235/300\n",
      "120/120 [==============================] - 0s 90us/step - loss: 0.3221 - acc: 0.8750 - val_loss: 0.3653 - val_acc: 0.9333\n",
      "Epoch 236/300\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.3214 - acc: 0.8750 - val_loss: 0.3645 - val_acc: 0.9333\n",
      "Epoch 237/300\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.3207 - acc: 0.8750 - val_loss: 0.3637 - val_acc: 0.9333\n",
      "Epoch 238/300\n",
      "120/120 [==============================] - 0s 73us/step - loss: 0.3201 - acc: 0.8750 - val_loss: 0.3628 - val_acc: 0.9333\n",
      "Epoch 239/300\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.3194 - acc: 0.8750 - val_loss: 0.3620 - val_acc: 0.9333\n",
      "Epoch 240/300\n",
      "120/120 [==============================] - 0s 78us/step - loss: 0.3187 - acc: 0.8750 - val_loss: 0.3611 - val_acc: 0.9333\n",
      "Epoch 241/300\n",
      "120/120 [==============================] - 0s 61us/step - loss: 0.3180 - acc: 0.8833 - val_loss: 0.3603 - val_acc: 0.9333\n",
      "Epoch 242/300\n",
      "120/120 [==============================] - 0s 95us/step - loss: 0.3175 - acc: 0.8833 - val_loss: 0.3595 - val_acc: 0.9333\n",
      "Epoch 243/300\n",
      "120/120 [==============================] - 0s 82us/step - loss: 0.3167 - acc: 0.8833 - val_loss: 0.3586 - val_acc: 0.9333\n",
      "Epoch 244/300\n",
      "120/120 [==============================] - 0s 57us/step - loss: 0.3162 - acc: 0.8833 - val_loss: 0.3578 - val_acc: 0.9333\n",
      "Epoch 245/300\n",
      "120/120 [==============================] - 0s 81us/step - loss: 0.3155 - acc: 0.8833 - val_loss: 0.3570 - val_acc: 0.9333\n",
      "Epoch 246/300\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.3148 - acc: 0.8833 - val_loss: 0.3562 - val_acc: 0.9333\n",
      "Epoch 247/300\n",
      "120/120 [==============================] - 0s 73us/step - loss: 0.3142 - acc: 0.8833 - val_loss: 0.3553 - val_acc: 0.9333\n",
      "Epoch 248/300\n",
      "120/120 [==============================] - 0s 65us/step - loss: 0.3136 - acc: 0.8833 - val_loss: 0.3545 - val_acc: 0.9000\n",
      "Epoch 249/300\n",
      "120/120 [==============================] - 0s 82us/step - loss: 0.3129 - acc: 0.8833 - val_loss: 0.3537 - val_acc: 0.9000\n",
      "Epoch 250/300\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.3123 - acc: 0.8833 - val_loss: 0.3528 - val_acc: 0.9000\n",
      "Epoch 251/300\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.3116 - acc: 0.8833 - val_loss: 0.3520 - val_acc: 0.9000\n",
      "Epoch 252/300\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.3110 - acc: 0.8833 - val_loss: 0.3512 - val_acc: 0.9000\n",
      "Epoch 253/300\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.3104 - acc: 0.8833 - val_loss: 0.3504 - val_acc: 0.9000\n",
      "Epoch 254/300\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.3098 - acc: 0.8833 - val_loss: 0.3496 - val_acc: 0.9000\n",
      "Epoch 255/300\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.3091 - acc: 0.8833 - val_loss: 0.3487 - val_acc: 0.9000\n",
      "Epoch 256/300\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.3085 - acc: 0.8833 - val_loss: 0.3479 - val_acc: 0.9000\n",
      "Epoch 257/300\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.3079 - acc: 0.8833 - val_loss: 0.3471 - val_acc: 0.9000\n",
      "Epoch 258/300\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.3073 - acc: 0.8833 - val_loss: 0.3464 - val_acc: 0.9000\n",
      "Epoch 259/300\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.3067 - acc: 0.8833 - val_loss: 0.3456 - val_acc: 0.9000\n",
      "Epoch 260/300\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.3061 - acc: 0.8833 - val_loss: 0.3448 - val_acc: 0.9000\n",
      "Epoch 261/300\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.3055 - acc: 0.8833 - val_loss: 0.3440 - val_acc: 0.9000\n",
      "Epoch 262/300\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.3049 - acc: 0.8833 - val_loss: 0.3432 - val_acc: 0.9000\n",
      "Epoch 263/300\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.3043 - acc: 0.8833 - val_loss: 0.3424 - val_acc: 0.9000\n",
      "Epoch 264/300\n",
      "120/120 [==============================] - 0s 96us/step - loss: 0.3037 - acc: 0.8833 - val_loss: 0.3417 - val_acc: 0.9000\n",
      "Epoch 265/300\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.3031 - acc: 0.8833 - val_loss: 0.3409 - val_acc: 0.9000\n",
      "Epoch 266/300\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.3025 - acc: 0.8833 - val_loss: 0.3401 - val_acc: 0.9000\n",
      "Epoch 267/300\n",
      "120/120 [==============================] - 0s 77us/step - loss: 0.3019 - acc: 0.8833 - val_loss: 0.3394 - val_acc: 0.9000\n",
      "Epoch 268/300\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.3013 - acc: 0.8833 - val_loss: 0.3386 - val_acc: 0.9000\n",
      "Epoch 269/300\n",
      "120/120 [==============================] - 0s 75us/step - loss: 0.3008 - acc: 0.8833 - val_loss: 0.3378 - val_acc: 0.9000\n",
      "Epoch 270/300\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.3001 - acc: 0.8833 - val_loss: 0.3370 - val_acc: 0.9000\n",
      "Epoch 271/300\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.2996 - acc: 0.8833 - val_loss: 0.3363 - val_acc: 0.9000\n",
      "Epoch 272/300\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.2990 - acc: 0.8833 - val_loss: 0.3355 - val_acc: 0.9000\n",
      "Epoch 273/300\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.2984 - acc: 0.8833 - val_loss: 0.3347 - val_acc: 0.9000\n",
      "Epoch 274/300\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.2978 - acc: 0.8833 - val_loss: 0.3339 - val_acc: 0.9000\n",
      "Epoch 275/300\n",
      "120/120 [==============================] - 0s 91us/step - loss: 0.2972 - acc: 0.8833 - val_loss: 0.3331 - val_acc: 0.9000\n",
      "Epoch 276/300\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.2965 - acc: 0.8833 - val_loss: 0.3323 - val_acc: 0.9000\n",
      "Epoch 277/300\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.2959 - acc: 0.8833 - val_loss: 0.3315 - val_acc: 0.9000\n",
      "Epoch 278/300\n",
      "120/120 [==============================] - 0s 95us/step - loss: 0.2952 - acc: 0.8833 - val_loss: 0.3307 - val_acc: 0.9000\n",
      "Epoch 279/300\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.2946 - acc: 0.8833 - val_loss: 0.3299 - val_acc: 0.9000\n",
      "Epoch 280/300\n",
      "120/120 [==============================] - 0s 89us/step - loss: 0.2940 - acc: 0.8833 - val_loss: 0.3291 - val_acc: 0.9000\n",
      "Epoch 281/300\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.2934 - acc: 0.8833 - val_loss: 0.3283 - val_acc: 0.9000\n",
      "Epoch 282/300\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.2928 - acc: 0.8833 - val_loss: 0.3275 - val_acc: 0.9000\n",
      "Epoch 283/300\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.2921 - acc: 0.8833 - val_loss: 0.3267 - val_acc: 0.9000\n",
      "Epoch 284/300\n",
      "120/120 [==============================] - 0s 81us/step - loss: 0.2915 - acc: 0.8833 - val_loss: 0.3259 - val_acc: 0.9000\n",
      "Epoch 285/300\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.2909 - acc: 0.8833 - val_loss: 0.3251 - val_acc: 0.9000\n",
      "Epoch 286/300\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.2902 - acc: 0.8833 - val_loss: 0.3244 - val_acc: 0.9000\n",
      "Epoch 287/300\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.2896 - acc: 0.8833 - val_loss: 0.3236 - val_acc: 0.9000\n",
      "Epoch 288/300\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.2890 - acc: 0.8833 - val_loss: 0.3228 - val_acc: 0.9000\n",
      "Epoch 289/300\n",
      "120/120 [==============================] - 0s 74us/step - loss: 0.2884 - acc: 0.8833 - val_loss: 0.3220 - val_acc: 0.9000\n",
      "Epoch 290/300\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.2878 - acc: 0.8833 - val_loss: 0.3212 - val_acc: 0.9000\n",
      "Epoch 291/300\n",
      "120/120 [==============================] - 0s 94us/step - loss: 0.2872 - acc: 0.8833 - val_loss: 0.3205 - val_acc: 0.9000\n",
      "Epoch 292/300\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.2865 - acc: 0.8833 - val_loss: 0.3197 - val_acc: 0.9000\n",
      "Epoch 293/300\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.2860 - acc: 0.8833 - val_loss: 0.3189 - val_acc: 0.9000\n",
      "Epoch 294/300\n",
      "120/120 [==============================] - 0s 90us/step - loss: 0.2853 - acc: 0.8833 - val_loss: 0.3181 - val_acc: 0.9000\n",
      "Epoch 295/300\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.2847 - acc: 0.8833 - val_loss: 0.3174 - val_acc: 0.9000\n",
      "Epoch 296/300\n",
      "120/120 [==============================] - 0s 87us/step - loss: 0.2841 - acc: 0.8833 - val_loss: 0.3166 - val_acc: 0.9000\n",
      "Epoch 297/300\n",
      "120/120 [==============================] - 0s 82us/step - loss: 0.2835 - acc: 0.8833 - val_loss: 0.3158 - val_acc: 0.9000\n",
      "Epoch 298/300\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.2829 - acc: 0.8833 - val_loss: 0.3150 - val_acc: 0.9000\n",
      "Epoch 299/300\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.2823 - acc: 0.8833 - val_loss: 0.3143 - val_acc: 0.9000\n",
      "Epoch 300/300\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.2816 - acc: 0.8833 - val_loss: 0.3135 - val_acc: 0.9000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x266a8bd2c88>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "iris = pd.read_csv('./iris.csv')\n",
    "iris.head()\n",
    "x = iris.drop('species',axis=1)\n",
    "y = iris['species']\n",
    "\n",
    "#encoding y\n",
    "from sklearn .preprocessing import LabelBinarizer\n",
    "encoder = LabelBinarizer()\n",
    "y = encoder.fit_transform(y)\n",
    "y\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(x)\n",
    "x\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test,y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=101)\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import cross_val_score\n",
    "model = Sequential()\n",
    "model.add(Dense(units= 4,input_shape=[4,],activation='relu'))\n",
    "model.add(Dense(units= 3, kernel_initializer='normal', activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(patience=10)\n",
    "\n",
    "model.fit(x=x_train,y=y_train,epochs=300,\n",
    "         validation_data=(x_test,y_test),callbacks=[early_stop])\n",
    "\n",
    "\n",
    "#iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.039904</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.048454</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.031486</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.039015</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.023193</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.029561</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.014738</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.019547</td>\n",
       "      <td>0.508333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.006072</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.009778</td>\n",
       "      <td>0.491667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.997357</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.999299</td>\n",
       "      <td>0.483333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.988461</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.989388</td>\n",
       "      <td>0.483333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.979500</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.979226</td>\n",
       "      <td>0.491667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.970554</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.968317</td>\n",
       "      <td>0.508333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.961325</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.958882</td>\n",
       "      <td>0.641667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.952146</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.948025</td>\n",
       "      <td>0.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.943074</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.937061</td>\n",
       "      <td>0.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.933849</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.926820</td>\n",
       "      <td>0.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.924804</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.915616</td>\n",
       "      <td>0.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.915613</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.905294</td>\n",
       "      <td>0.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.906369</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.895124</td>\n",
       "      <td>0.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.897349</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.883916</td>\n",
       "      <td>0.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.888483</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.873225</td>\n",
       "      <td>0.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.879493</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.863016</td>\n",
       "      <td>0.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.870628</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.852928</td>\n",
       "      <td>0.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.861855</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.842314</td>\n",
       "      <td>0.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.853305</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.831917</td>\n",
       "      <td>0.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.844774</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.822047</td>\n",
       "      <td>0.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.836291</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.812615</td>\n",
       "      <td>0.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.828038</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.802011</td>\n",
       "      <td>0.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.820033</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.792506</td>\n",
       "      <td>0.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.812184</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.783106</td>\n",
       "      <td>0.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.804372</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.773827</td>\n",
       "      <td>0.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.796775</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.764598</td>\n",
       "      <td>0.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.789335</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.755857</td>\n",
       "      <td>0.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>0.336253</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.299565</td>\n",
       "      <td>0.883333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>0.335499</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.298970</td>\n",
       "      <td>0.883333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>0.334736</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.298397</td>\n",
       "      <td>0.883333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>0.333943</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.297800</td>\n",
       "      <td>0.883333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>0.333139</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.297158</td>\n",
       "      <td>0.883333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>0.332339</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.296505</td>\n",
       "      <td>0.883333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>0.331535</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.295888</td>\n",
       "      <td>0.883333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>0.330739</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.295223</td>\n",
       "      <td>0.883333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>0.329931</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.294627</td>\n",
       "      <td>0.883333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>0.329123</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.293980</td>\n",
       "      <td>0.883333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>0.328326</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.293371</td>\n",
       "      <td>0.883333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>0.327511</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.292759</td>\n",
       "      <td>0.883333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>0.326720</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.292085</td>\n",
       "      <td>0.883333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>0.325923</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.291488</td>\n",
       "      <td>0.883333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>0.325135</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.290855</td>\n",
       "      <td>0.883333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>0.324358</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.290213</td>\n",
       "      <td>0.883333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>0.323560</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.289638</td>\n",
       "      <td>0.883333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>0.322784</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.289012</td>\n",
       "      <td>0.883333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>0.322016</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.288437</td>\n",
       "      <td>0.883333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>0.321235</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.287793</td>\n",
       "      <td>0.883333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>0.320462</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.287172</td>\n",
       "      <td>0.883333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>0.319687</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.286531</td>\n",
       "      <td>0.883333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>0.318899</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.285956</td>\n",
       "      <td>0.883333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>0.318135</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.285318</td>\n",
       "      <td>0.883333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>0.317361</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.284723</td>\n",
       "      <td>0.883333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>0.316588</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.284094</td>\n",
       "      <td>0.883333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>0.315816</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.283507</td>\n",
       "      <td>0.883333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.315044</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.282877</td>\n",
       "      <td>0.883333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.314258</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.282330</td>\n",
       "      <td>0.883333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.313499</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.281649</td>\n",
       "      <td>0.883333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     val_loss   val_acc      loss       acc\n",
       "0    1.039904  0.500000  1.048454  0.500000\n",
       "1    1.031486  0.500000  1.039015  0.500000\n",
       "2    1.023193  0.500000  1.029561  0.500000\n",
       "3    1.014738  0.500000  1.019547  0.508333\n",
       "4    1.006072  0.500000  1.009778  0.491667\n",
       "5    0.997357  0.466667  0.999299  0.483333\n",
       "6    0.988461  0.500000  0.989388  0.483333\n",
       "7    0.979500  0.566667  0.979226  0.491667\n",
       "8    0.970554  0.566667  0.968317  0.508333\n",
       "9    0.961325  0.600000  0.958882  0.641667\n",
       "10   0.952146  0.600000  0.948025  0.675000\n",
       "11   0.943074  0.600000  0.937061  0.675000\n",
       "12   0.933849  0.600000  0.926820  0.675000\n",
       "13   0.924804  0.600000  0.915616  0.675000\n",
       "14   0.915613  0.600000  0.905294  0.675000\n",
       "15   0.906369  0.600000  0.895124  0.675000\n",
       "16   0.897349  0.600000  0.883916  0.675000\n",
       "17   0.888483  0.600000  0.873225  0.675000\n",
       "18   0.879493  0.600000  0.863016  0.675000\n",
       "19   0.870628  0.600000  0.852928  0.675000\n",
       "20   0.861855  0.600000  0.842314  0.675000\n",
       "21   0.853305  0.600000  0.831917  0.675000\n",
       "22   0.844774  0.600000  0.822047  0.675000\n",
       "23   0.836291  0.600000  0.812615  0.675000\n",
       "24   0.828038  0.600000  0.802011  0.675000\n",
       "25   0.820033  0.600000  0.792506  0.675000\n",
       "26   0.812184  0.600000  0.783106  0.675000\n",
       "27   0.804372  0.600000  0.773827  0.675000\n",
       "28   0.796775  0.600000  0.764598  0.675000\n",
       "29   0.789335  0.600000  0.755857  0.675000\n",
       "..        ...       ...       ...       ...\n",
       "270  0.336253  0.900000  0.299565  0.883333\n",
       "271  0.335499  0.900000  0.298970  0.883333\n",
       "272  0.334736  0.900000  0.298397  0.883333\n",
       "273  0.333943  0.900000  0.297800  0.883333\n",
       "274  0.333139  0.900000  0.297158  0.883333\n",
       "275  0.332339  0.900000  0.296505  0.883333\n",
       "276  0.331535  0.900000  0.295888  0.883333\n",
       "277  0.330739  0.900000  0.295223  0.883333\n",
       "278  0.329931  0.900000  0.294627  0.883333\n",
       "279  0.329123  0.900000  0.293980  0.883333\n",
       "280  0.328326  0.900000  0.293371  0.883333\n",
       "281  0.327511  0.900000  0.292759  0.883333\n",
       "282  0.326720  0.900000  0.292085  0.883333\n",
       "283  0.325923  0.900000  0.291488  0.883333\n",
       "284  0.325135  0.900000  0.290855  0.883333\n",
       "285  0.324358  0.900000  0.290213  0.883333\n",
       "286  0.323560  0.900000  0.289638  0.883333\n",
       "287  0.322784  0.900000  0.289012  0.883333\n",
       "288  0.322016  0.900000  0.288437  0.883333\n",
       "289  0.321235  0.900000  0.287793  0.883333\n",
       "290  0.320462  0.900000  0.287172  0.883333\n",
       "291  0.319687  0.900000  0.286531  0.883333\n",
       "292  0.318899  0.900000  0.285956  0.883333\n",
       "293  0.318135  0.900000  0.285318  0.883333\n",
       "294  0.317361  0.900000  0.284723  0.883333\n",
       "295  0.316588  0.900000  0.284094  0.883333\n",
       "296  0.315816  0.900000  0.283507  0.883333\n",
       "297  0.315044  0.900000  0.282877  0.883333\n",
       "298  0.314258  0.900000  0.282330  0.883333\n",
       "299  0.313499  0.900000  0.281649  0.883333\n",
       "\n",
       "[300 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = pd.DataFrame(model.history.history)\n",
    "metrics\n",
    "metrics[['loss','val_loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x266a3c62278>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XuYFPWd7/H3d3ruzHCZGRDkOiiKoiA4Korxhho0jxKNzwY3N3Ny5ORiXEl2TzQa4xrdbPYk2U1WD/uQxI26bgiagyFn8RIjSI5RA0QUAQcRVAbkMswMt7n15Xf+6AKbYYbpmenu6q7+vJ5nnu6urq76FjV85te/qvqVOecQEZFgKfC7ABERST2Fu4hIACncRUQCSOEuIhJACncRkQBSuIuIBJDCXUQkgBTuIiIBpHAXEQmgQr9WXFNT4yZMmODX6kVEctLatWsbnXPDe5vPt3CfMGECa9as8Wv1IiI5yczeT2Y+dcuIiASQwl1EJIAU7iIiAeRbn3t3wuEwDQ0NtLe3+11KViotLWXMmDEUFRX5XYqIZLmsCveGhgYqKyuZMGECZuZ3OVnFOce+fftoaGigtrbW73JEJMtlVbdMe3s71dXVCvZumBnV1dX6ViMiScmqcAcU7CegfxsRSVZWdcuISI44sBP+8hjEoulbR0EhzPg8DB6VvnUEmMJdRPru9f+Ald8H0vlt0kGoCD72jTSuI7gU7iLSd4cboXQI3PlB+tbx4Cho3Ze+5Qdc1vW5Z4NPfvKTnHvuuUyZMoVFixYB8OyzzzJjxgymTZvG7NmzATh06BBf/OIXOfvss5k6dSq/+c1v/CxbJHPamqFsWHrXUTYM2lrSu44Ay9qW+9//bgMbdx5I6TLPPHkw371uSq/zPfLII1RVVdHW1sZ5553H3LlzufXWW1m1ahW1tbU0NTUB8L3vfY8hQ4awfv16AJqbm1Nar0jWamuCsqr0rqOsKr4e6ZesDXc//fSnP2Xp0qUAbN++nUWLFnHJJZccPb+8qir+S/3CCy+wePHio58bNizNLRmRbJGRlvvQ+HqkX7I23JNpYafDypUreeGFF3jllVcoLy/nsssuY9q0adTX1x83r3NOpydKfmptgqqJ6V1HeRXseTu96wgw9bl3sX//foYNG0Z5eTlvv/02r776Kh0dHbz00kts27YN4Gi3zNVXX81DDz109LPqlpG8kbE+d/2f6i+Fexdz5swhEokwdepUvvOd7zBz5kyGDx/OokWLuPHGG5k2bRqf/vSnAbjnnntobm7mrLPOYtq0aaxYscLn6kUyIBaF9v0ZCvcmcC696wmorO2W8UtJSQnPPPNMt+9dc801x7yuqKjg0UcfzURZItmjfT/gMnNANRaBzkNQUpnedQWQWu4i0jet3hksmWi5J65P+kThLiJ9c6QfvDzNLfcjy1e/e78kFe5mNsfM6s1si5nd2c37483sD2b2ppmtNLMxqS9VRLJCW4Zb7jrXvV96DXczCwEPA9cAZwI3m9mZXWb7IfCYc24qcD/w/VQXKiJZ4khLOu3hrpb7QCRzQPV8YItzbiuAmS0G5gIbE+Y5E1jgPV8BPJ3KIkUkDfZuhu2v9v1z21bFHzPVct/8PHQeTu+6Mm3sBTD89LSuIplwHw1sT3jdAFzQZZ43gE8BPwFuACrNrNo5d8yoP2Y2H5gPMG7cuP7WLCKpsPybHwV1X5VXQ+nQ1NZz3Dqq4gH/5uL4T5B84sdZEe7dXYLZ9cTTvwUeMrNbgFXADiBy3IecWwQsAqirq9PJqyJ+OrQHTr0KrvuXvn+2dCgUpPl8jFAR3LHeO/UyYNL9h5Hkwr0BGJvwegywM3EG59xO4EYAM6sAPuWcC+AeOVZFRQWHDh3yuwyR/mlrjncPDMni8x9KKnWOez8l86d3NTDJzGrNrBiYByxLnMHMaszsyLLuAh5JbZkiklLOxc8fT3e/ufim15a7cy5iZrcBzwEh4BHn3AYzux9Y45xbBlwGfN/MHPFuma8NuLJn7oRd6we8mGOMPBuu+cce3/7Wt77F+PHj+epXvwrAfffdh5mxatUqmpubCYfDPPDAA8ydO7fXVR06dIi5c+d2+7nHHnuMH/7wh5gZU6dO5fHHH2f37t18+ctfZuvWrQAsXLiQiy66KAUbLdKNzsMQC6f/XHXxTVLDDzjnlgPLu0y7N+H5U8BTqS0t8+bNm8cdd9xxNNyXLFnCs88+y4IFCxg8eDCNjY3MnDmT66+/vtfRIEtLS1m6dOlxn9u4cSMPPvggL7/8MjU1NUcHIbv99tu59NJLWbp0KdFoVN09kl6ZOlddfJO9Y8ucoIWdLtOnT2fPnj3s3LmTvXv3MmzYMEaNGsWCBQtYtWoVBQUF7Nixg927dzNy5MgTLss5x7e//e3jPvfiiy9y0003UVNTA3w0NvyLL77IY489BkAoFGLIkCHp3VjJb5k6Vz0Jf9rSSEtb2O8yMmrKyYMZXz0orevI3nD3yU033cRTTz3Frl27mDdvHk888QR79+5l7dq1FBUVMWHCBNrb23tdTk+f0xjwkhWOjg/jb7fM+ob9/PXPX/O1Bj888MmzFO6ZNm/ePG699VYaGxt56aWXWLJkCSNGjKCoqIgVK1bw/vvvJ7Wc/fv3d/u52bNnc8MNN7BgwQKqq6tpamqiqqqK2bNns3DhQu644w6i0SiHDx9m8ODB6dxUyWdZ0nL/w9u7MYPffOUiBhXnTxydNLgk7evIn3/NJE2ZMoWDBw8yevRoRo0axWc+8xmuu+466urqOOecc5g8eXJSy+npc1OmTOHuu+/m0ksvJRQKMX36dH75y1/yk5/8hPnz5/OLX/yCUCjEwoULufDCC9O5qZLPjvS5+3xAdUX9Xs4ZO5QZ4/zvHgoacz4NhF9XV+fWrFlzzLRNmzZxxhln+FJPrtC/kURjjq898Rd2tLT1exl/1bqYz7U9zo1VSwlbcQqr65u3du5nwZWncfvsSb7VkGvMbK1zrq63+dRyF8kxDc2tPLthF1NOHsxJg0v7tYwR4TY62ksZOtjfC4Q+fuZIPnVuFl9ElcMU7gO0fv16Pve5zx0zraSkhNdey7+DRJIZ2xrjg2h997opnF/bz26VpY/AthoeueW8FFYm2STrwj3XziY5++yzWbduXUbW5VcXmgxQNAzbX4s/pkBH/S5mFbzPpMOF8G4/u1Sa3vX9YKqkV1aFe2lpKfv27aO6ujqnAj4TnHPs27eP0tL+fQ0XH725BH771ZQt7uPAx4sZ+GWDp13T+zySs7Iq3MeMGUNDQwN79+71u5SsVFpaypgx6p/MOQd2xB9v+S+w0IAX98B/bWJ/Wyf/66ZpA1vQiOTO/JLclFXhXlRURG1trd9liKRWWzMUV8KEi1OyuN8fbues0UNg/IyULE+CKavCXSSQ2pr71L99oD3M3z35Bgfbj7slAgDbm1q5burJqapOAirNo+2LSHxo3eRvzvD7Dbt5bsNuDndGCUdjx/2cX1vFx6eceGwjEbXcRdKtrblPV4Ku3LyX4ZUlLP3KRRQU6MQC6R+Fu0iCltZO3t/X2u/PD68s4eShZcdMc21NHCgewXvbW3r9vANWbd7LVWeepGCXAVG4iyS45d9Xsy6JEO5JRUkhq+++krLij86K6Tiwj2W72/nOxpeTXs6VZ4zodw0ioHAXOWr3gXbWbW/h5vPHcdWZfQ/Xd3Yf4vvPvM2rW/dx+WTv87EYxeH9uNJhPHJTr8OBAFBaGGLmxOo+r18kkcJdxPNSffz6is9fOJ4zRvV9uOVZp9bwLy+8w4r6PUfDve1QC2XEGDlyFFdMPiml9YqciMJd8sKPnq9n8+6DJ5xn04cHGTm4lMkj+zeYVklhiFmnVvPbdTvZfSB+Q5fSgx/wE2D8WF18JpmlcJfAa2hu5V9f3MLooWVUlvb8K19eHOKWiyYMaOiLz84cz46W9qMHZU+LxMdNn6hwlwxTuEvgrfS6Wx770vmcMrwireu67PQRXHZ6Qn/9O53wBBRV1KR1vSJdKdyD7NAeaD/gdxW+2/TWei4aeoiJ9iE0Zvj0wr1vxx81AqNkmMI9qA7ugh+fCS7qdyW+e/DIk4f8qsBgkFruklkK96Da3xAP9ll/Ayed7Xc1vtm06wALV77L/7hkIlNOHuJPEZUjfb9XqeQfhXtQtXo3QJ58HYzN37vtPPn+Rp4rOJkfXHE1FA98uF2RXKFwD6q25vhjilqMT7++g5e3NKZkWZm0on4PMydWH3PFqEg+ULgHVZvXck/BgbxINMa9v30L5zjhqYTZqKQwxM3nj/O7DJGMy63/qZK8tmbAoHTg/cyvb2/hQHuE//2ZGVx79qiB1yYiaadwD6rWpniwF8S7I1o7Ixzu6N+ZM8++tYtQgXHxJJ3xIZIrFO5BlTCG+L5DHXzsn1bQ2tn/0yIvqK1icGlRqqoTkTRTuAdVW9PR/vaV9Xtp7Yyy4MrTqKoo7tfiLj5VrXaRXKJwD6q2ZiiPB/KK+j0Mryzh61ecqhtAiOQJhXtAudYm6sMjWfK7jbxUv5c5Z41UsIvkkaRukG1mc8ys3sy2mNmd3bw/zsxWmNnrZvammV2b+lKlL6KHm3jlwxiLV39AYci4Yfpov0sSkQzqteVuZiHgYeAqoAFYbWbLnHMbE2a7B1jinFtoZmcCy4EJaahXkhGNUBg+yEGrZO09V+kCHpE8lEy3zPnAFufcVgAzWwzMBRLD3QFHbl0zBNiZyiIDK9KR+kVGY0QPN1ICDKs5ScEukqeSCffRwPaE1w3ABV3muQ943sy+DgwCrkxJdUH2ysPw3LdTvthCPtqpY8eMTfnyRSQ3JBPu3R2Fc11e3wz80jn3IzO7EHjczM5yzsWOWZDZfGA+wLhxeX5JeONmKK6Aj30jZYt8bVsTL9XvZdap1YSKyzjv6ptTtmwRyS3JhHsDkNgEHMPx3S5fAuYAOOdeMbNSoAbYkziTc24RsAigrq6u6x+I/BLpjJ+H/rFvpmyR/7zpFVqGh/mfX7wkZcsUkdyUzNkyq4FJZlZrZsXAPGBZl3k+AGYDmNkZQCmwN5WFBk60E0KpueKzMxJj0ap3WfNeM5dPHtH7B0Qk8HoNd+dcBLgNeA7YRPysmA1mdr+ZXe/N9k3gVjN7A/gVcItzLr9b5r2JdkKof1eLdvWndxv5h+VvEyowPqGBvUSEJC9ics4tJ356Y+K0exOebwRmpba0gIuGU9Zy39Z4GIA/futyRlSWpmSZIpLbkrqISdIghS339xoPM6g4xPCKkpQsT0Ryn8LdL9FOCKUmjLfta2VCzSDMNLyAiMQp3P2SwgOq7zUeZkLNoJQsS0SCQeHulxR1y3RGYjQ0t1JbrXAXkY9oVEi/RMMDCvf2cJSn1jaw52AHMYda7iJyDIW7XwbYLbPsjZ3c8/RbABQWGNPGDPxeqSISHAp3vwywW2Zl/R5OGlzC83dcSnFhgQYIE5FjqM/dL9EwFPYv3MPRGH98p5HLTx/BkPIiBbuIHEctd7/0o+W+bnsLX/mPtbSHoxxsj3DZ6RpqQES6p3D3S6Tv4f706ztoOtzJTeeOoaK0kMsnD09TcSKS6xTufunHAdWV9Xu46JRqHrzh7DQVJSJBoXD3S5LdMq2dEZ7bsIvmw2He29fKf7u4NgPFiUiuU7j7IRYFF01q+IHHXnmff3zmbQCKQsYVGtJXRJKgcPdDNBx/TKJb5sW39zB5ZCU//0IdFSWFDC1PzWBjIhJsOhXSD9HO+GMv3TL728Ksfb+ZK884iTHDyhXsIpI0tdz94LXc39zVyh0/Wnn8HWk97eEo0ZjjstN1VoyI9I3C3Q/RDgD+tO0Ah9ojXDCxusdZr6ksYfq4YZmqTEQCQuHuB69b5t2mTuZdOpZvXH26zwWJSNCoz90PXrdMhyviUl1lKiJpoHD3g9dyLygs5pyxQ30uRkSCSOHuBy/cS0tLCRXo1ngiknoKdz943TJFxbqhtYikh8LdD5H42TJFJaU+FyIiQaVw94PXLVNcrHAXkfRQuPvB65YpVstdRNJE4e6HhAOqIiLpoHD3gTsa7mU+VyIiQaVw90FHRzuglruIpI/C3Qcd7Qp3EUkvhbsPOjraACgvH+RzJSISVAp3HxzplikvU5+7iKSHwt0HnV64V5Qr3EUkPRTuPoh0xq9QrSgv97kSEQmqpMLdzOaYWb2ZbTGzO7t5/5/NbJ33s9nMWlJfanCEwx1EnVFRprFlRCQ9er1Zh5mFgIeBq4AGYLWZLXPObTwyj3NuQcL8Xwemp6HWwIh0dhCmkMpS3StFRNIjmZb7+cAW59xW51wnsBiYe4L5bwZ+lYrigioa7qCTIgYVK9xFJD2SSZfRwPaE1w3ABd3NaGbjgVrgxYGXluM2PM2uPz3Bzpb2494a2bqZsBVSoLHcRSRNkgn37hLI9TDvPOAp51y02wWZzQfmA4wbNy6pAnPWnxcxZOdaDsaGUxQ69p8wbMU0VM2k59tii4gMTDLh3gCMTXg9BtjZw7zzgK/1tCDn3CJgEUBdXV1PfyCCobWJP7lpvDDtR3z/xrOPe3tC5isSkTySTJ/7amCSmdWaWTHxAF/WdSYzOx0YBryS2hJzU6ytiT2RcmprdLqjiGRer+HunIsAtwHPAZuAJc65DWZ2v5ldnzDrzcBi51ywW+TJcA5am2mhggnVGmJARDIvqdM1nHPLgeVdpt3b5fV9qSsrx4VbKYh10uIqqK1RuItI5ukK1XRobQKghQrGVqlbRkQyTydaD8CTa7az5r3m46aPanuHO4CC8ipKi0KZL0xE8p7CvZ86IlHu/e0GCguMQSXH/jPWxT4AYMop4/0oTURE4d5fr21toi0c5d9vOY/LJ4849s0NB+BJ+OzlGoVBRPwR6HA/2B6mMxJLy7Kf37iL4sICZk7s5lIkr8+dsmFpWbeISG8CG+5v7djP9Q/9P2JpPDHz0tOGU1bcTZ96m9cPr3AXEZ8ENty3Nh4m5uD2K06lpjI9Q+teetrw7t9oa4aicijSPVJFxB+BDff9rZ0AfPbC8YyozHDItjVDWVVm1ykikiCw57k3t4YBGFpWnPmVtzWrS0ZEfBXYcG9pDTOoOERxoQ+b2NoE5Qp3EfFPcMO9rZOh5T602kEtdxHxXXDDvTXM0PIif1be1qQ+dxHxVYDDvZNhfrTcnVPLXUR8F+BwDzPEj5Z7x0GIRRTuIuKr4IZ7W5hhfoT7kQuYytUtIyL+CWS4x2KOltZOn06D1NADIuK/QIb7wY4IMYc/B1SPDj2glruI+CeQ4b7/yAVMfhxQ1aBhIpIFAhnuzd7QA+pzF5F8Fchwb2mLt9yHlPkR7i3xx9KhmV+3iIgnkOHeEY4C+HOLu7YmKK6EQp+ujhURIaDhHvEGcS8K+bB5uoBJRLJAIMM9HI3ffakwZJlfuQYNE5EsEMhwj3ot98ICH8JdLXcRyQKBDPdI1At3X7plmhTuIuK7YIa77y13nQYpIv4KaLh7fe6ZDvdYTN0yIpIVAhnuYb+6ZToOgIvpAiYR8V0gwz3qV8tdg4aJSJYIZLh/1HLPdLhr0DARyQ6BDPePToXM8Oa1Hgl3tdxFxF+BDPdINIYZhDLeLaNBw0QkOwQy3MMxR1GmW+2Q0C2jlruI+CuQ4R6Nucy32uGjA6oaEVJEfJZUuJvZHDOrN7MtZnZnD/P8lZltNLMNZvafqS2zb8LRmD/jyrQ1Q8kQCBVmft0iIgl6TSEzCwEPA1cBDcBqM1vmnNuYMM8k4C5glnOu2cxGpKvgZESizp+rUzVomIhkiWSamOcDW5xzWwHMbDEwF9iYMM+twMPOuWYA59yeVBfaF9FohFn2JmzqzOyKGzerv11EskIy4T4a2J7wugG4oMs8pwGY2ctACLjPOfds1wWZ2XxgPsC4ceP6U29Sag+s5tbo9+DXaVtFz8643oeViogcK5lw765/w3WznEnAZcAY4I9mdpZzruWYDzm3CFgEUFdX13UZKVPR2Rh/8tdPQuXIdK2me9WnZHZ9IiLdSCbcG4CxCa/HADu7medV51wY2GZm9cTDfnVKquyj0sj++JNxM6F0sB8liIj4KpmzZVYDk8ys1syKgXnAsi7zPA1cDmBmNcS7abamstC+KI0cIEoBlFT6VYKIiK96DXfnXAS4DXgO2AQscc5tMLP7zexIB/NzwD4z2wisAP7OObcvXUX3pjyyn4NWCebDGTMiIlkgqROynXPLgeVdpt2b8NwB3/B+fFcWPcDBgkp0KZGI5KtAXqE6KHqAQwXqkhGR/BXQcD/I4QIdSBWR/BXIcK+IHVC4i0heC2S4D3IHORxSuItI/gpeuEc6KHPttBcq3EUkfwUv3L0x1VsLh/hciIiIf4IX7q3xMdXVcheRfBascHcOtr8KQJta7iKSx4IV7u++CP93AQCtxTU+FyMi4p9g3TLo0G4AHij8GgfKa30uRkTEP8FquYfbAPijm07Ijxtki4hkiWAlYKQdgNZYIUV+3ENVRCRLBCvcvZb74VgRIT/uoSoikiWCF+5WQGusgKJQsDZNRKQvgpWAkXYoLCMag0K13EUkjwUr3MNtuKJSwlGncBeRvBascI+0Q2EpAIXqlhGRPBasBAy34grLACjU2TIikscCFu7tuCIv3NUtIyJ5LFjhHmnDhbxuGV3EJCJ5LFgJGG4ndrTPXS13EclfwQr3SBuxUAmglruI5LdgJWC4jZgOqIqIBC3c24kVHOlzV7iLSP4KVrhH2oge6ZbRee4ikseClYDh9o/CXS13EcljwQr3SBuRkM5zFxEJTrhHwxCLEC2It9w1KqSI5LPgJKA3lnvEC3eN5y4i+Sw44e7dhSlccOSAqsJdRPJXcML9aMtdww+IiAQnAdVyFxE5KqfD/UB7mGVv7Iy/CLfGH8w7oKqWu4jksaQS0MzmmFm9mW0xszu7ef8WM9trZuu8n/+e+lKP9/M/buP2X73OB/taIRxvuR+KFQJQVhzKRAkiIlmpsLcZzCwEPAxcBTQAq81smXNuY5dZf+2cuy0NNfZoZf0eALY2HmJcYbzPveGQUWAwtqosk6WIiGSVZFru5wNbnHNbnXOdwGJgbnrL6t3egx282bAfgPcaDx9tuX9wIMbJQ8soKVTLXUTyV68td2A0sD3hdQNwQTfzfcrMLgE2Awucc9u7mWfAXv3P7zG1/l8pBzbEu9fpeKka5twFwHv7o9TWDErHqkVEckYy4d7daSeuy+vfAb9yznWY2ZeBR4ErjluQ2XxgPsC4ceP6WGpc5fgZvLn/RgDKSwo5/OFmLux8DXa9BcCmlgIunqBwF5H8lky4NwBjE16PAXYmzuCc25fw8mfAD7pbkHNuEbAIoK6urusfiKRMmfUJmPWJo69/+rOfceGO16DpXQB2tpcwQS13EclzyfS5rwYmmVmtmRUD84BliTOY2aiEl9cDm1JX4okNrRoBgNv3LtHCMjooZkJ1eaZWLyKSlXptuTvnImZ2G/AcEAIecc5tMLP7gTXOuWXA7WZ2PRABmoBb0ljzMUaOGgXrgZYPaC8ZDsCkEZWZWr2ISFZKplsG59xyYHmXafcmPL8LuCu1pSXn3MmnwPNgLsq+6CDGV5frNEgRyXs5fxln9bAqwt7fqB2dpVx++gjMNPSAiOS3nA93zOgsGgxAU2wQl50+3OeCRET8l/vhDhRXxgP95FEnM+vUGp+rERHxXyDCvaiiCoDpp03UHZhERAhIuFNW5T0O87cOEZEsEZBwH3bso4hIngtGuJd7oV5e5W8dIiJZIhjhrpa7iMgxAhLu6nMXEUmU1BWqWe/0a6HlA6g5ze9KRESyQjDCvfIkuPK7flchIpI1gtEtIyIix1C4i4gEkMJdRCSAFO4iIgGkcBcRCSCFu4hIACncRUQCSOEuIhJA5pzzZ8Vme4H3+/nxGqAxheX4SduSnbQt2UnbAuOdc73ecs63cB8IM1vjnKvzu45U0LZkJ21LdtK2JE/dMiIiAaRwFxEJoFwN90V+F5BC2pbspG3JTtqWJOVkn7uIiJxYrrbcRUTkBHIu3M1sjpnVm9kWM7vT73r6yszeM7P1ZrbOzNZ406rM7Pdm9o73mJW3lDKzR8xsj5m9lTCt29ot7qfefnrTzGb4V/nxetiW+8xsh7dv1pnZtQnv3eVtS72Zfdyfqo9nZmPNbIWZbTKzDWb2N970nNsvJ9iWXNwvpWb2ZzN7w9uWv/em15rZa95++bWZFXvTS7zXW7z3Jwy4COdczvwAIeBdYCJQDLwBnOl3XX3chveAmi7T/gm403t+J/ADv+vsofZLgBnAW73VDlwLPAMYMBN4ze/6k9iW+4C/7WbeM73ftRKg1vsdDPm9DV5to4AZ3vNKYLNXb87tlxNsSy7uFwMqvOdFwGvev/cSYJ43/d+Ar3jPvwr8m/d8HvDrgdaQay3384EtzrmtzrlOYDEw1+eaUmEu8Kj3/FHgkz7W0iPn3CqgqcvknmqfCzzm4l4FhprZqMxU2rsetqUnc4HFzrkO59w2YAvx30XfOec+dM79xXt+ENgEjCYH98sJtqUn2bxfnHPukPeyyPtxwBXAU970rvvlyP56CphtZjaQGnIt3EcD2xNeN3DinZ+NHPC8ma01s/netJOccx9C/BccGOFbdX3XU+25uq9u87orHknoHsuJbfG+yk8n3krM6f3SZVsgB/eLmYXMbB2wB/g98W8WLc65iDdLYr1Ht8V7fz9QPZD151q4d/eXLNdO95nlnJsBXAN8zcwu8bugNMnFfbUQOAU4B/gQ+JE3Peu3xcwqgN8AdzjnDpxo1m6mZfu25OR+cc5FnXPnAGOIf6M4o7vZvMeUb0uuhXsDMDbh9Rhgp0+19Itzbqf3uAdYSnyn7z7y1dh73ONfhX3WU+05t6+cc7u9/5Ax4Gd89BU/q7fFzIqIh+ETzrn/403Oyf3S3bbk6n45wjnXAqwk3uc+1MwKvbcS6z26Ld77Q0i+27BbuRbuq4FJ3hHnYuIHHpb5XFPSzGyQmVUeeQ5cDbxFfBu+4M32BeC3/lTYLz3Vvgz4vHd2xkxg/5FugmzVpe/5BuL7BuLbMs87o6EWmAT8OdP1dcfrl/0FsMk59+OEt3Juv/S0LTm6X4ab2VBsjQ+kAAAA5ElEQVTveRlwJfFjCCuAm7zZuu6XI/vrJuBF5x1d7Te/jyr34yj0tcSPor8L3O13PX2sfSLxo/tvABuO1E+8b+0PwDveY5XftfZQ/6+Ify0OE29pfKmn2ol/zXzY20/rgTq/609iWx73an3T+882KmH+u71tqQeu8bv+hLouJv71/U1gnfdzbS7ulxNsSy7ul6nA617NbwH3etMnEv8DtAV4Eijxppd6r7d4708caA26QlVEJIByrVtGRESSoHAXEQkghbuISAAp3EVEAkjhLiISQAp3EZEAUriLiASQwl1EJID+P0689DW1Tvi+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics[['acc','val_acc']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3134986460208893, 0.8999999761581421]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test,y_test,verbose=0) # final loss against final accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 1.0810 - acc: 0.5867\n",
      "Epoch 2/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 1.0769 - acc: 0.6067\n",
      "Epoch 3/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 1.0728 - acc: 0.5933\n",
      "Epoch 4/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 1.0683 - acc: 0.6000\n",
      "Epoch 5/300\n",
      "150/150 [==============================] - 0s 59us/step - loss: 1.0640 - acc: 0.6000\n",
      "Epoch 6/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 1.0590 - acc: 0.6000\n",
      "Epoch 7/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 1.0539 - acc: 0.6267\n",
      "Epoch 8/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 1.0488 - acc: 0.6333\n",
      "Epoch 9/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 1.0430 - acc: 0.6333\n",
      "Epoch 10/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 1.0369 - acc: 0.6333\n",
      "Epoch 11/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 1.0305 - acc: 0.6533\n",
      "Epoch 12/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 1.0238 - acc: 0.6533\n",
      "Epoch 13/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 1.0164 - acc: 0.6533\n",
      "Epoch 14/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 1.0090 - acc: 0.6600\n",
      "Epoch 15/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 1.0008 - acc: 0.6600\n",
      "Epoch 16/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.9926 - acc: 0.6600\n",
      "Epoch 17/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.9841 - acc: 0.6733\n",
      "Epoch 18/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.9752 - acc: 0.6800\n",
      "Epoch 19/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.9659 - acc: 0.6733\n",
      "Epoch 20/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.9562 - acc: 0.6733\n",
      "Epoch 21/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.9462 - acc: 0.6800\n",
      "Epoch 22/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.9363 - acc: 0.6800\n",
      "Epoch 23/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.9255 - acc: 0.6867\n",
      "Epoch 24/300\n",
      "150/150 [==============================] - 0s 59us/step - loss: 0.9150 - acc: 0.6867\n",
      "Epoch 25/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.9042 - acc: 0.6933\n",
      "Epoch 26/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.8930 - acc: 0.6933\n",
      "Epoch 27/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.8816 - acc: 0.6933\n",
      "Epoch 28/300\n",
      "150/150 [==============================] - 0s 59us/step - loss: 0.8705 - acc: 0.6933\n",
      "Epoch 29/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.8587 - acc: 0.7067\n",
      "Epoch 30/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.8473 - acc: 0.7067\n",
      "Epoch 31/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.8355 - acc: 0.7067\n",
      "Epoch 32/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.8240 - acc: 0.7067\n",
      "Epoch 33/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.8126 - acc: 0.7067\n",
      "Epoch 34/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.8006 - acc: 0.7067\n",
      "Epoch 35/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.7889 - acc: 0.7133\n",
      "Epoch 36/300\n",
      "150/150 [==============================] - 0s 59us/step - loss: 0.7776 - acc: 0.7133\n",
      "Epoch 37/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.7663 - acc: 0.7133\n",
      "Epoch 38/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.7547 - acc: 0.7200\n",
      "Epoch 39/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.7436 - acc: 0.7333\n",
      "Epoch 40/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.7327 - acc: 0.7400\n",
      "Epoch 41/300\n",
      "150/150 [==============================] - 0s 59us/step - loss: 0.7219 - acc: 0.7400\n",
      "Epoch 42/300\n",
      "150/150 [==============================] - 0s 65us/step - loss: 0.7111 - acc: 0.7400\n",
      "Epoch 43/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.7002 - acc: 0.7400\n",
      "Epoch 44/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.6903 - acc: 0.7400\n",
      "Epoch 45/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.6801 - acc: 0.7400\n",
      "Epoch 46/300\n",
      "150/150 [==============================] - 0s 59us/step - loss: 0.6705 - acc: 0.7533\n",
      "Epoch 47/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.6606 - acc: 0.7533\n",
      "Epoch 48/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.6511 - acc: 0.7600\n",
      "Epoch 49/300\n",
      "150/150 [==============================] - 0s 59us/step - loss: 0.6421 - acc: 0.7667\n",
      "Epoch 50/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.6331 - acc: 0.7733\n",
      "Epoch 51/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.6244 - acc: 0.7800\n",
      "Epoch 52/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.6160 - acc: 0.7867\n",
      "Epoch 53/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.6078 - acc: 0.7933\n",
      "Epoch 54/300\n",
      "150/150 [==============================] - 0s 59us/step - loss: 0.5998 - acc: 0.8000\n",
      "Epoch 55/300\n",
      "150/150 [==============================] - 0s 59us/step - loss: 0.5921 - acc: 0.8067\n",
      "Epoch 56/300\n",
      "150/150 [==============================] - 0s 65us/step - loss: 0.5845 - acc: 0.8133\n",
      "Epoch 57/300\n",
      "150/150 [==============================] - 0s 78us/step - loss: 0.5772 - acc: 0.8133\n",
      "Epoch 58/300\n",
      "150/150 [==============================] - 0s 59us/step - loss: 0.5700 - acc: 0.8133\n",
      "Epoch 59/300\n",
      "150/150 [==============================] - 0s 59us/step - loss: 0.5633 - acc: 0.8200\n",
      "Epoch 60/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.5565 - acc: 0.8200\n",
      "Epoch 61/300\n",
      "150/150 [==============================] - 0s 59us/step - loss: 0.5500 - acc: 0.8200\n",
      "Epoch 62/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.5437 - acc: 0.8200\n",
      "Epoch 63/300\n",
      "150/150 [==============================] - 0s 59us/step - loss: 0.5377 - acc: 0.8267\n",
      "Epoch 64/300\n",
      "150/150 [==============================] - 0s 59us/step - loss: 0.5319 - acc: 0.8267\n",
      "Epoch 65/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.5261 - acc: 0.8200\n",
      "Epoch 66/300\n",
      "150/150 [==============================] - 0s 59us/step - loss: 0.5205 - acc: 0.8200\n",
      "Epoch 67/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.5151 - acc: 0.8267\n",
      "Epoch 68/300\n",
      "150/150 [==============================] - 0s 59us/step - loss: 0.5101 - acc: 0.8267\n",
      "Epoch 69/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.5049 - acc: 0.8333\n",
      "Epoch 70/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.5000 - acc: 0.8333\n",
      "Epoch 71/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.4953 - acc: 0.8333\n",
      "Epoch 72/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.4907 - acc: 0.8333\n",
      "Epoch 73/300\n",
      "150/150 [==============================] - 0s 59us/step - loss: 0.4862 - acc: 0.8333\n",
      "Epoch 74/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.4819 - acc: 0.8333\n",
      "Epoch 75/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.4777 - acc: 0.8333\n",
      "Epoch 76/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.4736 - acc: 0.8400\n",
      "Epoch 77/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.4696 - acc: 0.8400\n",
      "Epoch 78/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.4657 - acc: 0.8400\n",
      "Epoch 79/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.4620 - acc: 0.8400\n",
      "Epoch 80/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.4584 - acc: 0.8400\n",
      "Epoch 81/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.4548 - acc: 0.8467\n",
      "Epoch 82/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.4513 - acc: 0.8467\n",
      "Epoch 83/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.4480 - acc: 0.8533\n",
      "Epoch 84/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.4447 - acc: 0.8467\n",
      "Epoch 85/300\n",
      "150/150 [==============================] - 0s 59us/step - loss: 0.4415 - acc: 0.8467\n",
      "Epoch 86/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.4384 - acc: 0.8467\n",
      "Epoch 87/300\n",
      "150/150 [==============================] - 0s 59us/step - loss: 0.4354 - acc: 0.8533\n",
      "Epoch 88/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.4323 - acc: 0.8533\n",
      "Epoch 89/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.4294 - acc: 0.8533\n",
      "Epoch 90/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.4266 - acc: 0.8533\n",
      "Epoch 91/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.4238 - acc: 0.8533\n",
      "Epoch 92/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.4212 - acc: 0.8533\n",
      "Epoch 93/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.4185 - acc: 0.8600\n",
      "Epoch 94/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.4158 - acc: 0.8600\n",
      "Epoch 95/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.4133 - acc: 0.8600\n",
      "Epoch 96/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.4108 - acc: 0.8600\n",
      "Epoch 97/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.4084 - acc: 0.8600\n",
      "Epoch 98/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.4062 - acc: 0.8600\n",
      "Epoch 99/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.4040 - acc: 0.8600\n",
      "Epoch 100/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.4014 - acc: 0.8667\n",
      "Epoch 101/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.3991 - acc: 0.8667\n",
      "Epoch 102/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.3969 - acc: 0.8667\n",
      "Epoch 103/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.3948 - acc: 0.8667\n",
      "Epoch 104/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.3927 - acc: 0.8667\n",
      "Epoch 105/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.3907 - acc: 0.8667\n",
      "Epoch 106/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.3885 - acc: 0.8667\n",
      "Epoch 107/300\n",
      "150/150 [==============================] - 0s 59us/step - loss: 0.3865 - acc: 0.8667\n",
      "Epoch 108/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.3845 - acc: 0.8733\n",
      "Epoch 109/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.3826 - acc: 0.8733\n",
      "Epoch 110/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.3806 - acc: 0.8733\n",
      "Epoch 111/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.3787 - acc: 0.8667\n",
      "Epoch 112/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.3768 - acc: 0.8667\n",
      "Epoch 113/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.3750 - acc: 0.8733\n",
      "Epoch 114/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.3732 - acc: 0.8733\n",
      "Epoch 115/300\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.3215 - acc: 0.937 - 0s 52us/step - loss: 0.3714 - acc: 0.8733\n",
      "Epoch 116/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.3697 - acc: 0.8733\n",
      "Epoch 117/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.3679 - acc: 0.8733\n",
      "Epoch 118/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.3662 - acc: 0.8733\n",
      "Epoch 119/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.3649 - acc: 0.8667\n",
      "Epoch 120/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.3629 - acc: 0.8667\n",
      "Epoch 121/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.3613 - acc: 0.8667\n",
      "Epoch 122/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.3596 - acc: 0.8733\n",
      "Epoch 123/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.3581 - acc: 0.8733\n",
      "Epoch 124/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.3565 - acc: 0.8733\n",
      "Epoch 125/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.3549 - acc: 0.8733\n",
      "Epoch 126/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.3533 - acc: 0.8733\n",
      "Epoch 127/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.3518 - acc: 0.8800\n",
      "Epoch 128/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.3502 - acc: 0.8733\n",
      "Epoch 129/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.3487 - acc: 0.8800\n",
      "Epoch 130/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.3473 - acc: 0.8800\n",
      "Epoch 131/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.3458 - acc: 0.8800\n",
      "Epoch 132/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.3444 - acc: 0.8800\n",
      "Epoch 133/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.3430 - acc: 0.8800\n",
      "Epoch 134/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.3415 - acc: 0.8800\n",
      "Epoch 135/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.3401 - acc: 0.8800\n",
      "Epoch 136/300\n",
      "150/150 [==============================] - 0s 59us/step - loss: 0.3387 - acc: 0.8800\n",
      "Epoch 137/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.3373 - acc: 0.8800\n",
      "Epoch 138/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.3360 - acc: 0.8867\n",
      "Epoch 139/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.3346 - acc: 0.8867\n",
      "Epoch 140/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.3332 - acc: 0.8867\n",
      "Epoch 141/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.3319 - acc: 0.8867\n",
      "Epoch 142/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.3306 - acc: 0.8933\n",
      "Epoch 143/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.3292 - acc: 0.8933\n",
      "Epoch 144/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.3280 - acc: 0.8933\n",
      "Epoch 145/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.3267 - acc: 0.8933\n",
      "Epoch 146/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.3254 - acc: 0.8933\n",
      "Epoch 147/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.3241 - acc: 0.8933\n",
      "Epoch 148/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.3229 - acc: 0.8933\n",
      "Epoch 149/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.3216 - acc: 0.8933\n",
      "Epoch 150/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.3204 - acc: 0.8933\n",
      "Epoch 151/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.3192 - acc: 0.8933\n",
      "Epoch 152/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.3180 - acc: 0.8933\n",
      "Epoch 153/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.3167 - acc: 0.8933\n",
      "Epoch 154/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.3158 - acc: 0.8933\n",
      "Epoch 155/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.3144 - acc: 0.8933\n",
      "Epoch 156/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.3131 - acc: 0.8933\n",
      "Epoch 157/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.3120 - acc: 0.8933\n",
      "Epoch 158/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.3109 - acc: 0.8933\n",
      "Epoch 159/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.3097 - acc: 0.8933\n",
      "Epoch 160/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.3085 - acc: 0.8933\n",
      "Epoch 161/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.3074 - acc: 0.8933\n",
      "Epoch 162/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.3062 - acc: 0.8933\n",
      "Epoch 163/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.3050 - acc: 0.8933\n",
      "Epoch 164/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.3040 - acc: 0.8933\n",
      "Epoch 165/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.3028 - acc: 0.8933\n",
      "Epoch 166/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 52us/step - loss: 0.3017 - acc: 0.8933\n",
      "Epoch 167/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.3006 - acc: 0.8933\n",
      "Epoch 168/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2994 - acc: 0.8933\n",
      "Epoch 169/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2983 - acc: 0.8933\n",
      "Epoch 170/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.2973 - acc: 0.8933\n",
      "Epoch 171/300\n",
      "150/150 [==============================] - 0s 33us/step - loss: 0.2962 - acc: 0.8933\n",
      "Epoch 172/300\n",
      "150/150 [==============================] - 0s 59us/step - loss: 0.2952 - acc: 0.8933\n",
      "Epoch 173/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.2940 - acc: 0.8933\n",
      "Epoch 174/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.2930 - acc: 0.8933\n",
      "Epoch 175/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.2919 - acc: 0.8933\n",
      "Epoch 176/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.2909 - acc: 0.8933\n",
      "Epoch 177/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.2898 - acc: 0.8933\n",
      "Epoch 178/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2888 - acc: 0.8933\n",
      "Epoch 179/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2877 - acc: 0.8933\n",
      "Epoch 180/300\n",
      "150/150 [==============================] - 0s 65us/step - loss: 0.2867 - acc: 0.8933\n",
      "Epoch 181/300\n",
      "150/150 [==============================] - 0s 65us/step - loss: 0.2856 - acc: 0.8933\n",
      "Epoch 182/300\n",
      "150/150 [==============================] - 0s 72us/step - loss: 0.2846 - acc: 0.8933\n",
      "Epoch 183/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.2836 - acc: 0.8933\n",
      "Epoch 184/300\n",
      "150/150 [==============================] - 0s 65us/step - loss: 0.2826 - acc: 0.9000\n",
      "Epoch 185/300\n",
      "150/150 [==============================] - 0s 78us/step - loss: 0.2816 - acc: 0.9000\n",
      "Epoch 186/300\n",
      "150/150 [==============================] - 0s 59us/step - loss: 0.2805 - acc: 0.9000\n",
      "Epoch 187/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2795 - acc: 0.9000\n",
      "Epoch 188/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2786 - acc: 0.9000\n",
      "Epoch 189/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2775 - acc: 0.9000\n",
      "Epoch 190/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.2765 - acc: 0.9000\n",
      "Epoch 191/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2755 - acc: 0.9000\n",
      "Epoch 192/300\n",
      "150/150 [==============================] - 0s 59us/step - loss: 0.2745 - acc: 0.9067\n",
      "Epoch 193/300\n",
      "150/150 [==============================] - 0s 78us/step - loss: 0.2735 - acc: 0.9067\n",
      "Epoch 194/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.2725 - acc: 0.9067\n",
      "Epoch 195/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.2715 - acc: 0.9067\n",
      "Epoch 196/300\n",
      "150/150 [==============================] - 0s 65us/step - loss: 0.2707 - acc: 0.9067\n",
      "Epoch 197/300\n",
      "150/150 [==============================] - 0s 59us/step - loss: 0.2696 - acc: 0.9067\n",
      "Epoch 198/300\n",
      "150/150 [==============================] - 0s 59us/step - loss: 0.2687 - acc: 0.9067\n",
      "Epoch 199/300\n",
      "150/150 [==============================] - 0s 85us/step - loss: 0.2676 - acc: 0.9067\n",
      "Epoch 200/300\n",
      "150/150 [==============================] - 0s 85us/step - loss: 0.2667 - acc: 0.9067\n",
      "Epoch 201/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2657 - acc: 0.9067\n",
      "Epoch 202/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.2647 - acc: 0.9067\n",
      "Epoch 203/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2638 - acc: 0.9067\n",
      "Epoch 204/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2628 - acc: 0.9067\n",
      "Epoch 205/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.2618 - acc: 0.9067\n",
      "Epoch 206/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.2609 - acc: 0.9067\n",
      "Epoch 207/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2599 - acc: 0.9067\n",
      "Epoch 208/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.2590 - acc: 0.9067\n",
      "Epoch 209/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2580 - acc: 0.9133\n",
      "Epoch 210/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.2571 - acc: 0.9133\n",
      "Epoch 211/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2561 - acc: 0.9133\n",
      "Epoch 212/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2552 - acc: 0.9133\n",
      "Epoch 213/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.2543 - acc: 0.9133\n",
      "Epoch 214/300\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.2018 - acc: 0.937 - 0s 52us/step - loss: 0.2533 - acc: 0.9133\n",
      "Epoch 215/300\n",
      "150/150 [==============================] - 0s 59us/step - loss: 0.2525 - acc: 0.9133\n",
      "Epoch 216/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2515 - acc: 0.9133\n",
      "Epoch 217/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.2505 - acc: 0.9133\n",
      "Epoch 218/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2498 - acc: 0.9133\n",
      "Epoch 219/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.2488 - acc: 0.9133\n",
      "Epoch 220/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.2477 - acc: 0.9133\n",
      "Epoch 221/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2467 - acc: 0.9133\n",
      "Epoch 222/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2456 - acc: 0.9133\n",
      "Epoch 223/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2447 - acc: 0.9133\n",
      "Epoch 224/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2436 - acc: 0.9133\n",
      "Epoch 225/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.2425 - acc: 0.9200\n",
      "Epoch 226/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2416 - acc: 0.9200\n",
      "Epoch 227/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.2405 - acc: 0.9267\n",
      "Epoch 228/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.2395 - acc: 0.9267\n",
      "Epoch 229/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2385 - acc: 0.9267\n",
      "Epoch 230/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2375 - acc: 0.9267\n",
      "Epoch 231/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.2365 - acc: 0.9267\n",
      "Epoch 232/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2354 - acc: 0.9267\n",
      "Epoch 233/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.2344 - acc: 0.9267\n",
      "Epoch 234/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2334 - acc: 0.9267\n",
      "Epoch 235/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.2325 - acc: 0.9267\n",
      "Epoch 236/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2314 - acc: 0.9267\n",
      "Epoch 237/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2304 - acc: 0.9333\n",
      "Epoch 238/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.2294 - acc: 0.9333\n",
      "Epoch 239/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2283 - acc: 0.9333\n",
      "Epoch 240/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2274 - acc: 0.9333\n",
      "Epoch 241/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2264 - acc: 0.9333\n",
      "Epoch 242/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2254 - acc: 0.9333\n",
      "Epoch 243/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2244 - acc: 0.9333\n",
      "Epoch 244/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2235 - acc: 0.9333\n",
      "Epoch 245/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2225 - acc: 0.9333\n",
      "Epoch 246/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.2215 - acc: 0.9333\n",
      "Epoch 247/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2206 - acc: 0.9333\n",
      "Epoch 248/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2195 - acc: 0.9333\n",
      "Epoch 249/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2185 - acc: 0.9333\n",
      "Epoch 250/300\n",
      "150/150 [==============================] - 0s 33us/step - loss: 0.2176 - acc: 0.9333\n",
      "Epoch 251/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.2165 - acc: 0.9333\n",
      "Epoch 252/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2156 - acc: 0.9333\n",
      "Epoch 253/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.2147 - acc: 0.9333\n",
      "Epoch 254/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2136 - acc: 0.9333\n",
      "Epoch 255/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.2127 - acc: 0.9267\n",
      "Epoch 256/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.2117 - acc: 0.9267\n",
      "Epoch 257/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.2107 - acc: 0.9333\n",
      "Epoch 258/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2097 - acc: 0.9267\n",
      "Epoch 259/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.2088 - acc: 0.9200\n",
      "Epoch 260/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.2078 - acc: 0.9200\n",
      "Epoch 261/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2069 - acc: 0.9200\n",
      "Epoch 262/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2059 - acc: 0.9267\n",
      "Epoch 263/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.2049 - acc: 0.9333\n",
      "Epoch 264/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.2040 - acc: 0.9333\n",
      "Epoch 265/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2031 - acc: 0.9333\n",
      "Epoch 266/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.2021 - acc: 0.9333\n",
      "Epoch 267/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.2012 - acc: 0.9333\n",
      "Epoch 268/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.2003 - acc: 0.9333\n",
      "Epoch 269/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.1993 - acc: 0.9333\n",
      "Epoch 270/300\n",
      "150/150 [==============================] - 0s 59us/step - loss: 0.1985 - acc: 0.9333\n",
      "Epoch 271/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.1974 - acc: 0.9400\n",
      "Epoch 272/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.1966 - acc: 0.9400\n",
      "Epoch 273/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.1956 - acc: 0.9400\n",
      "Epoch 274/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.1947 - acc: 0.9400\n",
      "Epoch 275/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.1937 - acc: 0.9400\n",
      "Epoch 276/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.1929 - acc: 0.9400\n",
      "Epoch 277/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.1919 - acc: 0.9467\n",
      "Epoch 278/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.1910 - acc: 0.9467\n",
      "Epoch 279/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.1901 - acc: 0.9467\n",
      "Epoch 280/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.1893 - acc: 0.9467\n",
      "Epoch 281/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.1883 - acc: 0.9467\n",
      "Epoch 282/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.1875 - acc: 0.9467\n",
      "Epoch 283/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.1866 - acc: 0.9467\n",
      "Epoch 284/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.1857 - acc: 0.9467\n",
      "Epoch 285/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.1848 - acc: 0.9467\n",
      "Epoch 286/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.1840 - acc: 0.9467\n",
      "Epoch 287/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.1830 - acc: 0.9467\n",
      "Epoch 288/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.1822 - acc: 0.9467\n",
      "Epoch 289/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.1812 - acc: 0.9467\n",
      "Epoch 290/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.1804 - acc: 0.9467\n",
      "Epoch 291/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.1796 - acc: 0.9467\n",
      "Epoch 292/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.1787 - acc: 0.9467\n",
      "Epoch 293/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.1778 - acc: 0.9467\n",
      "Epoch 294/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.1770 - acc: 0.9467\n",
      "Epoch 295/300\n",
      "150/150 [==============================] - 0s 52us/step - loss: 0.1762 - acc: 0.9467\n",
      "Epoch 296/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.1752 - acc: 0.9467\n",
      "Epoch 297/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.1745 - acc: 0.9467\n",
      "Epoch 298/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.1736 - acc: 0.9467\n",
      "Epoch 299/300\n",
      "150/150 [==============================] - 0s 39us/step - loss: 0.1728 - acc: 0.9467\n",
      "Epoch 300/300\n",
      "150/150 [==============================] - 0s 46us/step - loss: 0.1719 - acc: 0.9467\n"
     ]
    }
   ],
   "source": [
    "# want the model to train on all the data -> no test train split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "iris = pd.read_csv('./iris.csv')\n",
    "iris.head()\n",
    "x = iris.drop('species',axis=1)\n",
    "y = iris['species']\n",
    "\n",
    "#encoding y\n",
    "from sklearn .preprocessing import LabelBinarizer\n",
    "encoder = LabelBinarizer()\n",
    "y = encoder.fit_transform(y)\n",
    "y\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(x)\n",
    "x\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import cross_val_score\n",
    "model = Sequential()\n",
    "model.add(Dense(units= 4,input_shape=[4,],activation='relu'))\n",
    "model.add(Dense(units= 3, kernel_initializer='normal', activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "epochs=300\n",
    "model.fit(x,y,epochs=epochs)\n",
    "\n",
    "\n",
    "model.save(\"final_iris_model.h5\")\n",
    "\n",
    "import joblib\n",
    "\n",
    "joblib.dump(scaler,'iris_scaler.pkl')\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "flower_model = load_model(\"final_iris_model.h5\")\n",
    "flower_scaler = joblib.load('iris_scaler.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Iris-setosa'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def return_prediction(model,scaler,sample_json):\n",
    "    \n",
    "    # if json has a lot of field loop throught the fields\n",
    "    s_len = sample_json[\"sepal_l\"]\n",
    "    s_wid = sample_json[\"sepal_w\"]\n",
    "    p_len = sample_json[\"petal_l\"]\n",
    "    p_wid = sample_json[\"petal_w\"]\n",
    "    \n",
    "    flower = [[s_len,s_wid,p_len,p_wid]]\n",
    "    classes = np.array(['Iris-setosa','Iris-versicolor','Iris-virginica'])\n",
    "    flower = scaler.transform(flower)\n",
    "    class_id = model.predict_classes(flower)[0]\n",
    "    return classes[class_id]\n",
    "\n",
    "flower_example = {\"sepal_l\":5.1,\"sepal_w\":3.5,\"petal_l\":1.4,\"petal_w\":0.2}\n",
    "\n",
    "return_prediction(flower_model,flower_scaler,flower_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
